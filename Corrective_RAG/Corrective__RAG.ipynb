{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Corrective RAG (CRAG) Implementation from Scratch\n",
    "\n",
    "Welcome to this notebook! Here, you'll find a **from-scratch implementation** of the Corrective RAG (CRAG) framework, inspired by the research paper:  \n",
    "[Corrective RAG: Faithful and Efficient Retrieval-Augmented Generation via Corrective Reasoning](https://arxiv.org/pdf/2401.15884)\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What is Corrective RAG?\n",
    "\n",
    "Corrective RAG (CRAG) is an advanced Retrieval-Augmented Generation (RAG) approach that enhances factual accuracy and reliability in LLM-based question answering. It introduces a **corrective reasoning loop** that verifies, refines, and supplements retrieved knowledge before generating a final answer.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Implementation Highlights\n",
    "\n",
    "- **Built from scratch**: No high-level CRAG libraries used—every component is custom-coded for transparency and flexibility.\n",
    "- **Google Gemini & Embeddings**: Uses `langchain-google-genai` and `llama_index.embeddings.google_genai` for LLM and embedding models.\n",
    "- **Vector Store Indexing**: Documents are split, embedded, and indexed for efficient retrieval.\n",
    "- **Factuality Evaluation**: Combines similarity scoring and LLM-based factual checks to classify answers as `CORRECT`, `INCORRECT`, or `AMBIGUOUS`.\n",
    "- **Web Search Integration**: For ambiguous or incorrect cases, queries are rewritten and external web search is performed (via Serper API).\n",
    "- **Knowledge Refinement**: Retrieved passages are split into fine-grained \"knowledge strips\" and filtered for relevance using the LLM.\n",
    "- **Final Answer Generation**: The LLM generates a response strictly based on the refined internal and/or external knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 Project Structure\n",
    "\n",
    "- **Document Loading & Indexing**: Loads documents from the `data/` directory, splits them into nodes, and builds a vector index.\n",
    "- **Factuality Agent**: Evaluates if retrieved passages support the query, invoking LLM checks when uncertain.\n",
    "- **Query Rewriting & Web Search**: Rewrites queries for optimal web search and fetches up-to-date information.\n",
    "- **Knowledge Refinement**: Extracts and recomposes only the most relevant knowledge strips for answer generation.\n",
    "- **CRAG Inference Pipeline**: Orchestrates the above steps to deliver factually accurate and context-aware answers.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Example Usage\n",
    "\n",
    "- Ask a question like:  \n",
    "    *\"What is the difference between Flat white and cappuccino?\"*\n",
    "- The system retrieves, verifies, and refines knowledge, then generates a grounded answer.\n",
    "- If internal knowledge is insufficient, it automatically supplements with web search.\n",
    "\n",
    "---\n",
    "## CITATION \n",
    "```bibtex\n",
    "@misc{yan2024correctiveretrievalaugmentedgeneration,\n",
    "      title={Corrective Retrieval Augmented Generation}, \n",
    "      author={Shi-Qi Yan and Jia-Chen Gu and Yun Zhu and Zhen-Hua Ling},\n",
    "      year={2024},\n",
    "      eprint={2401.15884},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL},\n",
    "      url={https://arxiv.org/abs/2401.15884}, \n",
    "}\n",
    "```\n",
    "---\n",
    "\n",
    "## 📖 References\n",
    "\n",
    "- [Corrective RAG Paper (arXiv:2401.15884)](https://arxiv.org/pdf/2401.15884)\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Get Started\n",
    "\n",
    "1. Place your documents in the `data/` folder.\n",
    "2. Set your API keys for Google and Serper.\n",
    "3. Run the notebook cells in order.\n",
    "4. Try your own queries and explore the corrective reasoning process!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy experimenting with CRAG!** 🦾✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCE : [Corrective RAG paper](https://arxiv.org/pdf/2401.15884)\n",
    "![Corrective RAG Algorithm](https://raw.githubusercontent.com/prasanna00019/RAG-Playground/main/Corrective_RAG/flow.png)\n",
    "\n",
    "The flowchart of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index.llms.langchain\n",
    "# !pip install langchain_community\n",
    "# %pip install llama-index-embeddings-google-genai\n",
    "# !pip install llama_index\n",
    "import os\n",
    "GOOGLE_API_KEY = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# 2. Setup embedding and LLM\n",
    "embed_model = GoogleGenAIEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    embed_batch_size=100,\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "# 3. Create vector index\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_based_factual_check(query, chunks,web_context=\"\"):\n",
    "  if(web_context==\"\"):\n",
    "    prompt = f\"\"\"\n",
    "    You are a factual evaluator. Given the query:\n",
    "    \"{query}\"\n",
    "\n",
    "    And the following retrieved passages:\n",
    "    {chunks}\n",
    "\n",
    "    Decide if the retrieved passages sufficiently support the query.\n",
    "    If you think you don't know the answer to this query, return \"DONTKNOW\".\n",
    "    else return only one word: CORRECT, INCORRECT, or AMBIGUOUS.\n",
    "    \"\"\"\n",
    "  else:\n",
    "    prompt = f\"\"\"\n",
    "    You are a factual evaluator. Given the query:\n",
    "    \"{query}\"\n",
    "\n",
    "    And the following retrieved passages:\n",
    "    {chunks}\n",
    "\n",
    "    And the following retrieved web context:\n",
    "    {web_context}\n",
    "\n",
    "    Decide if the retrieved passages sufficiently support the query.\n",
    "    else return only one word: CORRECT, INCORRECT, or AMBIGUOUS.\n",
    "    \"\"\"\n",
    "  result = llm.invoke(prompt)\n",
    "  return result.content.strip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llmverdict:  CORRECT\n",
      "[0.6414507745301866, 0.6274194287931348, 0.6058399572764629, 0.5943758868876117, 0.5675433654179854] CORRECT The preparation method is a key difference between a Flat White and a Cappuccino. For a Flat White, the coffee is added to the cup first, followed by warm milk, and the milk foam is prepared last, lying under the crema and taking on its color and taste. In contrast, for a Cappuccino, the hot milk and milk foam are prepared first, and then the coffee is added, flowing through the milk foam to create a characteristic white foam top.\n",
      "\n",
      "Regarding ingredients and machine settings, a Flat White typically uses 60 ml of espresso with machine settings for 2 seconds of milk foam and 14 seconds of warm milk, along with normal coffee strength and temperature. A Cappuccino uses 50 ml of espresso, with machine settings for 14 seconds of milk foam, and strong coffee strength and high temperature.\n"
     ]
    }
   ],
   "source": [
    "# tool\n",
    "def evaluator_agent(query, vector_index, top_k=5, correct_thresh=0.75, incorrect_thresh=0.4):\n",
    "    \"\"\"\n",
    "    Takes in a query and returns a score,confidence level and response.\n",
    "    \"\"\"\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=top_k)\n",
    "    response = query_engine.query(query)\n",
    "    similarity_scores = [node.score for node in response.source_nodes]\n",
    "\n",
    "    # If similarity looks uncertain, ask LLM to verify\n",
    "    if all(score >= correct_thresh for score in similarity_scores):\n",
    "        return similarity_scores, \"CORRECT\", response\n",
    "    elif all(score <= incorrect_thresh for score in similarity_scores):\n",
    "        return similarity_scores, \"INCORRECT\", response\n",
    "    else:\n",
    "        # Optional: LLM-based re-verification\n",
    "        llm_verdict = llm_based_factual_check(query, [node.node.text for node in response.source_nodes])\n",
    "        print(\"llmverdict: \",llm_verdict)\n",
    "        if(llm_verdict==\"DONTKNOW\"):\n",
    "          rewritten_q = query_rewriter(query)\n",
    "          web_context = search_web(rewritten_q)\n",
    "          final_confidence = llm_based_factual_check(query, [node.node.text for node in response.source_nodes],web_context)\n",
    "          return similarity_scores, final_confidence, response\n",
    "        else:\n",
    "          return similarity_scores, llm_verdict, response\n",
    "\n",
    "s,c,response=evaluator_agent(\"What is the difference between Flat white and cappuccino?\",vector_index)\n",
    "print(s,c,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6022679957581402, 0.5665247476663722, 0.566452335371869, 0.5534099582177929, 0.5119777212800196] INCORRECT The flat white originated in Australia. There is no information indicating that it originated in Italy.\n"
     ]
    }
   ],
   "source": [
    "query_2=\"What is the history of the flat white and how did it originate in Italy? ?\"\n",
    "scores_2,c,response=evaluator_agent(query_2,vector_index)\n",
    "print(scores_2,c,response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool\n",
    "def generator_agent(query,knowledge):\n",
    "  \"\"\"\n",
    "  Takes in a query and knowledge and returns a response.\n",
    "  \"\"\"\n",
    "  prompt = f\"\"\"\n",
    "  You are a helpful and factually accurate assistant.\n",
    "  Answer the following user query based strictly on the provided context.\n",
    "  Avoid making up information, and mention if something is not found in the context.\n",
    "\n",
    "  User Query:\n",
    "  {query}\n",
    "\n",
    "  Context:\n",
    "  {knowledge}\n",
    "\n",
    "  Answer:\"\"\"\n",
    "  response = llm.invoke(prompt)\n",
    "  return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tool\n",
    "def query_rewriter(query):\n",
    "  \"\"\"\n",
    "  Takes in a query and returns a rewritten effective query for a web search engine.\n",
    "  \"\"\"\n",
    "  prompt = f\"\"\"\n",
    "You are a helpful assistant whose job is to rewrite user queries to make them more effective for a web search engine.\n",
    "\n",
    "Rewrite the following user query to make it clear, unambiguous, and optimized for retrieving accurate and up-to-date information from the web.\n",
    "\n",
    "Example 1:\n",
    "User Query: how did Tesla start?\n",
    "Rewritten Web Search Query: when and by whom was Tesla Motors founded?\n",
    "\n",
    "Example 2:\n",
    "User Query: effects of covid\n",
    "Rewritten Web Search Query: what are the long-term health effects of COVID-19 according to WHO?\n",
    "\n",
    "User Query:\n",
    "\"{query}\"\n",
    "Rewritten Web Search Query:\n",
    "\"\"\"\n",
    "  response = llm.invoke(prompt)\n",
    "  rewritten_query = response.content.strip().split(\"\\n\")[-1].replace(\"Rewritten Web Search Query:\", \"\").strip()\n",
    "  return rewritten_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tool\n",
    "def knowledge_refine(query, source_nodes=response, strip_size=2):\n",
    "    \"\"\"\n",
    "    Takes in the query and retrieved nodes.\n",
    "    Splits each document into strips, asks the LLM to score them in bulk, and returns relevant ones.\n",
    "    \"\"\"\n",
    "    combined_docs = \"\"\n",
    "    for i, node in enumerate(source_nodes.source_nodes):\n",
    "        combined_docs += f\"[Document {i+1}]\\n{node.text.strip()}\\n<end-of-document>\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert at extracting fine-grained knowledge segments from retrieved documents.\n",
    "\n",
    "Given the user query: \"{query}\"\n",
    "\n",
    "Instructions:\n",
    "1. For each document, break it into smaller \"knowledge strips\" of 1–3 sentences each.\n",
    "2. Score each strip on a scale from 0 to 1 for how relevant it is to the query.\n",
    "3. Keep only those strips with score >= 0.6.\n",
    "4. Recompose the selected strips **in order**, without repeating the score or document ID.\n",
    "\n",
    "Your output should be a clean, ordered recomposition of the relevant strips.\n",
    "\n",
    "Documents:\n",
    "{combined_docs}\n",
    "\n",
    "Final Refined Knowledge:\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=knowledge_refine(\"What is the difference between Flat white and cappuccino?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cappuccino, the coffee is prepared after the hot milk and milk foam, then flows through the milk foam at the top. The milk foam, which weighs less than the coffee, floats above it, creating the characteristic white foam top. The word cappuccino probably comes from the Capuchin monks, and came to be used because the milk foam resembles the monk’s hood, with the colour of the monks’ hoods also recalling the brown hue of the beverage. A cappuccino typically uses 50 ml espresso and milk foam, with machine settings often involving 14 seconds for milk foam.\n",
      "\n",
      "In contrast, the flat white is prepared by adding the coffee to the cup first, followed by warm milk. For flat white, the milk foam is prepared after the coffee and lies under the crema. This results in the milk foam at the top taking on the colour and flavour of the crema. A flat white typically uses 60 ml espresso, warm milk, and milk foam.\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPER_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "def search_web(inputs: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the web using Serper.dev.\n",
    "    \"\"\"\n",
    "    search = GoogleSerperAPIWrapper()\n",
    "    return search.run(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCE : [Corrective RAG paper](https://arxiv.org/pdf/2401.15884)\n",
    "![Corrective RAG Algorithm](https://raw.githubusercontent.com/prasanna00019/RAG-Playground/main/Corrective_RAG/algorithm.png)\n",
    "\n",
    "**This algorithm is implemented below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_CRAG_inference(query):\n",
    "  Internal_knowledge=\"\"\n",
    "  external_knowledge=\"\"\n",
    "  web_query=\"\"\n",
    "  k=\"\"\n",
    "  print(c,\" :c\")\n",
    "  print(\" \")\n",
    "  if(c==\"CORRECT\"):\n",
    "   Internal_knowledge= knowledge_refine(query)\n",
    "   k=Internal_knowledge\n",
    "  elif(c==\"INCORRECT\"):\n",
    "   web_query=query_rewriter(query)\n",
    "   external_knowledge=search_web(web_query)\n",
    "   k=external_knowledge\n",
    "  elif(c==\"AMBIGUOUS\"):\n",
    "    Internal_knowledge= knowledge_refine(query)\n",
    "    web_query=query_rewriter(query)\n",
    "    external_knowledge=search_web(web_query)\n",
    "    k=Internal_knowledge+external_knowledge\n",
    "    # print(\"internal knowledge: \",Internal_knowledge)\n",
    "    # print(\"external knowledge: \",external_knowledge,\" :end\")\n",
    "  # print(\"k: \",k)\n",
    "  print(\" \")\n",
    "  G=generator_agent(query,k)\n",
    "  return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCORRECT  :c\n",
      " \n",
      " \n",
      "The origin of the flat white is disputed, but it is widely accepted to have originated in Oceania, specifically either New Zealand or Australia, in the 1980s. Coffee historian Ian Bersten suggests it may have originated in England in the 1950s. In Australia, it was introduced as a balance between an intense espresso and a milky latte. Australian barista Alan Preston and New Zealander Derek Townsend are among those who claim to have invented the drink.\n",
      "\n",
      "The provided context does not state that the flat white originated in Italy. While it mentions \"Italian sugar growers in the Sunshine State are said to have inspired the 'invention' of the flat white,\" this does not indicate an Italian origin for the drink itself.\n"
     ]
    }
   ],
   "source": [
    "final_CRAG_output_2=algorithm_CRAG_inference(query_2)\n",
    "print(final_CRAG_output_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT  :c\n",
      " \n",
      " \n",
      "The main difference between a Flat white and a cappuccino, according to the context, lies in their preparation method and the resulting appearance of the milk foam:\n",
      "\n",
      "*   **Flat White:** The coffee is added to the cup first, followed by warm milk. The milk foam is prepared in the final stage and lies *under* the crema, taking on its colour and taste. This results in the milk foam at the top having the colour and flavour of the crema.\n",
      "*   **Cappuccino:** The milk is prepared first, then the coffee. The coffee is prepared after the hot milk and milk foam, flowing *through* the milk foam at the top. The milk foam, being lighter than the coffee, floats above it, creating a characteristic white foam top.\n"
     ]
    }
   ],
   "source": [
    "final_CRAG_output=algorithm_CRAG_inference(\"What is the difference between Flat white and cappuccino?\")\n",
    "print(final_CRAG_output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
