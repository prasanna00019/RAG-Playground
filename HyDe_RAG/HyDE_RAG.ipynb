{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyDe RAG: Hypothetical Document Enhanced Retrieval-Augmented Generation üöÄüìÑ\n",
    "\n",
    "This project implements **HyDe RAG** (Hypothetical Document Enhanced Retrieval-Augmented Generation), an advanced RAG pipeline that improves retrieval and answer generation by leveraging hypothetical answers. Instead of retrieving context solely based on the user query, HyDe RAG first generates one or more hypothetical answers to the query, embeds them, and uses these embeddings to retrieve the most relevant document chunks. This approach can improve retrieval quality, especially for complex or ambiguous queries.\n",
    "\n",
    "Inspired from the research paper of HyDe : [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/pdf/2212.10496)\n",
    "---\n",
    "\n",
    "## Features ‚ú®\n",
    "\n",
    "- **Single and Multiple HyDe Modes:**  \n",
    "    Generate one or several hypothetical answers for a query, and use their embeddings (averaged in the multi-hypo case) for retrieval. ü§î‚û°Ô∏èüìö\n",
    "- **Flexible LLM and Embedding Integration:**  \n",
    "    Uses Google Gemini for LLM and Google GenAI for embeddings. ü§ñüîó\n",
    "- **Semantic Scoring:**  \n",
    "    Retrieved chunks are scored for semantic similarity to the hypothetical answer(s). üß†üìà\n",
    "- **Chunked Document Processing:**  \n",
    "    Documents are split into manageable chunks for efficient retrieval and ranking. ‚úÇÔ∏èüìÑ\n",
    "- **Easy-to-Use Functions:**  \n",
    "    Includes `HydeRAG` and `HydeRAG_Multiple` for single and multi-hypothetical workflows. üõ†Ô∏è\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow üîÑ\n",
    "\n",
    "1. **Document Loading & Chunking:**  \n",
    "     Documents are loaded from the `data` directory and split into chunks using `SentenceSplitter`. üìÇ‚úÇÔ∏è\n",
    "\n",
    "2. **Embedding & LLM Setup:**  \n",
    "     Google GenAI embeddings and Gemini LLM are configured. üß©ü§ñ\n",
    "\n",
    "3. **Indexing:**  \n",
    "     Chunks are indexed using a vector store for fast similarity search. üóÇÔ∏è‚ö°\n",
    "\n",
    "4. **Hypothetical Answer Generation:**  \n",
    "     For a given query, the LLM generates one or more plausible answers (hypotheticals). üí°\n",
    "\n",
    "5. **Embedding Hypotheticals:**  \n",
    "     Each hypothetical answer is embedded. In multi-hypo mode, embeddings are averaged. üß¨‚ûó\n",
    "\n",
    "6. **Retrieval:**  \n",
    "     The hypothetical embedding(s) are used to retrieve the most relevant document chunks. üéØ\n",
    "\n",
    "7. **Scoring & Ranking:**  \n",
    "     Retrieved chunks are scored (cosine similarity or semantic evaluator) and ranked. üèÖ\n",
    "\n",
    "8. **Final Answer Generation:**  \n",
    "     The top-ranked chunks are provided as context to the LLM to generate the final answer. üìù\n",
    "\n",
    "---\n",
    "## üìñ References\n",
    "\n",
    "- [HyDe RAG Paper (arXiv:2212.10496)](https://arxiv.org/pdf/2212.10496)\n",
    "\n",
    "---\n",
    "# Custom Chunk Retrieval Implementation üõ†Ô∏è\n",
    "\n",
    "Unlike out-of-the-box retrieval solutions, this project implements chunk retrieval from scratch for maximum flexibility and transparency:\n",
    "\n",
    "- **Document Chunking:** Documents are split into manageable chunks using a sentence-based splitter.\n",
    "- **Embedding:** Both queries (or their hypothetical answers) and document chunks are embedded using Google GenAI.\n",
    "- **Similarity Scoring:** Retrieved chunks are scored using either cosine similarity or semantic evaluators, allowing for fine-grained ranking.\n",
    "- **Manual Ranking:** The code sorts and selects the top-k chunks based on similarity scores, ensuring only the most relevant context is used for answer generation.\n",
    "\n",
    "This custom approach allows for experimentation with different retrieval strategies, scoring methods, and chunking techniques, making the system highly adaptable and research-friendly.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to refer to the code cells for detailed implementation of each step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index.llms.langchain\n",
    "# !pip install langchain_community\n",
    "# %pip install llama-index-embeddings-google-genai\n",
    "import os\n",
    "GOOGLE_API_KEY = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "# jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --to notebook --inplace HyDe_RAG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama_index\n",
    "from llama_index.core import VectorStoreIndex, Settings, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "embed_model = GoogleGenAIEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    embed_batch_size=100,\n",
    "    api_key=\"\"  \n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm  \n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=5,embed_model=embed_model,embeddings=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_answer(query, llm):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Generate a short paragraph that might answer the user's question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Hypothetical Answer:\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=generate_hypothetical_answer(\"what is prompt engineering ?\",llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the process of designing, refining, and optimizing the inputs (prompts) given to artificial intelligence models, especially large language models (LLMs), to achieve desired outputs. It involves understanding how these models interpret and respond to text, then crafting precise instructions, examples, and context to guide their behavior, improve accuracy, and unlock specific capabilities for various tasks.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embed_model.get_text_embedding(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038928118, -0.021924319, -0.047387216, -0.024699317, 0.024130518, 0.047847446, 0.024508096, 0.03380397, -0.015584286, -0.0073079923, -0.04244941, 0.006492105, 0.029012477, -0.00811799, 0.023100581, -0.074430674, 0.018327028, 0.032765668, -0.1046555, 0.0075687724, 0.0008435602, -0.01599861, -0.045755155, -0.0812758, -0.017151818, 0.035411023, 0.030723568, -0.026784474, 0.0190255, -0.032942865, 0.06873405, 0.06443513, 0.044786695, -0.08529241, -0.01751432, -0.0015939691, 0.00023016837, 0.052878816, 0.050034266, -0.015983835, -0.062446117, 0.01304924, -0.049132917, 0.027779132, -0.012147965, 0.0061806957, -0.03554646, 0.037413374, -0.0369422, 0.03924568, 0.022033593, -0.03438433, -0.027625127, 0.030073874, -0.045050923, 0.0021168902, -0.028933953, 0.057786524, 0.031006405, 0.014710289, -0.0020874338, -0.047296442, -0.017321639, -0.024701672, 0.0041553415, -0.011589824, -0.033095412, -0.0635702, -0.06775403, 0.011659481, 0.013382095, 0.047499366, -0.022251086, 0.011511231, -0.026293436, -0.0031594464, 0.011820172, -0.0055713393, -0.037222203, 0.012658215, -0.0018045637, 0.007612647, 0.06921562, 0.08370818, -0.020868009, -0.042928465, 0.036890086, -0.0688467, -0.07924472, 0.0073867952, 0.017057657, -0.016324755, -0.011328344, 0.02217182, 0.06559169, 0.0138553055, -0.044572413, -0.052495092, 0.05638389, 0.060464982, -0.01090431, 0.035648994, 0.011838976, -0.0236243, 0.020372165, 0.08102385, -0.03865785, 0.013756228, -0.062658444, 0.03985081, -0.019977689, 0.030982684, 0.004353123, -0.027747402, 0.013634231, 0.005097368, -0.038274113, -0.0058030915, 0.002107354, 0.018768756, -0.026931597, 0.025130391, -0.022714155, 0.03824423, 0.05932343, 0.05099195, 0.06354369, -0.0051536015, -0.024886675, -0.024754029, 0.15803652, -0.06763255, -0.034689713, 0.041853067, -0.045230925, -0.04103424, 0.038971215, -0.0049805366, 0.046096545, 0.03549224, -0.0077733994, -0.016957348, -0.08240276, 0.004917601, 0.017498465, -0.004774273, 0.0025313415, 0.011996134, -0.010134791, 0.013467483, -0.03670282, -0.015224805, 0.03060362, 0.042206097, -0.016725076, -0.014921406, 0.036259163, -0.013327928, 0.051006332, 0.031893723, 0.046632577, -0.040271346, 0.009531794, -0.0013726392, -0.06783394, -0.028336588, 0.039970558, -0.03882144, -0.0048829224, 0.030110667, 0.0024804387, 0.035663683, 0.040463436, -0.10123669, 0.022867817, 0.017750053, -0.0120637035, -0.06563813, -0.05164322, 0.011452315, 0.08350582, 0.0685608, 0.0008365628, -0.045843996, -0.00012280223, 0.01208012, 0.0023498784, 0.0293967, 0.039888673, 0.055695277, 0.038276084, -0.02713368, 0.043963294, 0.0075769657, 0.03662165, -0.007303573, 0.0048683686, -0.067609884, 0.020677293, -0.055324413, 0.022664053, -0.026668737, -0.0012675844, -0.029566696, 0.001318349, 0.0041967654, -0.029563218, -0.05753956, -0.0144607155, 0.0044701663, -0.034729026, -0.007699859, -0.04322661, -0.085537255, 0.009124265, 0.0066032074, 0.076271944, -0.027937248, 0.10691021, -0.00060754607, 0.0038284862, -0.045628127, -0.027062628, 0.0062724156, 0.09175194, 0.08285273, -0.06895311, -0.0107326405, 0.014702117, -0.073479205, 0.01586304, 0.040144455, 0.01167883, 0.012499823, -0.00663783, 0.033810887, -0.0054995003, 0.016516788, -0.013096918, -0.004944436, -0.028336825, 0.041710023, -0.0053704283, -0.0029724156, 0.03911308, 0.01766578, 0.013098411, -0.0052389107, -0.021673257, -0.0891256, 0.016630623, -0.032290358, -0.0009986244, -0.024322584, -0.037654467, -0.057610482, 0.018523857, 0.0033591741, -0.0010747326, 0.012717767, 0.06434322, -0.06328039, -0.024341837, -0.06802051, 0.01578908, -0.083502874, 0.014385931, -0.008635279, 0.0038067442, -0.04362273, 0.018339299, -0.006485701, 0.00095705333, -0.024622023, -0.015759509, 0.025502145, 0.0035224515, 0.030864673, -0.07603733, -0.018703526, 0.014433627, 0.013467429, 0.04514465, -0.050941452, 0.046497002, -0.032214526, 0.025858233, 0.0013157076, 0.00047894786, -0.03737124, 0.055512212, 0.0067109955, -0.042364076, -0.014648162, 0.016447555, 0.0041027926, 0.038353465, 0.024088016, 0.007339121, 0.03970819, -0.004672396, 0.04999818, 0.005190295, 0.05456497, 0.029459646, 0.0459565, 0.025048004, -0.03065835, -0.06481116, 0.060156506, 0.006167784, 0.016253425, -0.03478186, -0.026418813, -0.00350943, 0.0029825363, -0.15737757, 0.016401077, -0.07639913, 0.0011975203, 0.0131600695, 0.018654145, -0.02768419, -0.0044153663, 0.023876812, -0.003744458, 0.0038516202, 0.042294204, 0.0021951064, 0.027260454, 0.028399928, 0.032294143, -0.012628884, -0.021250656, -0.008217728, -0.003577697, 0.0028728487, 0.03175722, 0.041989088, 0.036595434, -0.0069854627, 0.01854734, 0.066610046, 0.059622854, -0.057894636, 0.048638605, 0.032116354, -0.0062290314, 0.05178743, -0.013039976, 0.014415144, 0.0793216, 0.063458174, 0.0134715, -0.020627916, 0.011726888, 0.014582399, -0.026496137, 0.032936018, 0.021544306, 0.0050063254, 0.05482001, -0.022151595, 0.026217874, 0.030549303, -0.024539841, -0.020796997, 0.003516516, -0.023929037, 0.014930792, 0.019466342, -0.011467487, 0.009796144, -0.0222704, 0.03113088, -0.014455107, -0.0133582335, 0.027427526, -0.005429954, -0.007274904, -0.0026542775, -0.07456975, 0.0021728845, -0.002812687, -0.011448177, 0.062460292, -0.002259082, -0.045783598, 0.014064537, 0.015848465, -0.0031442062, 0.027533988, 0.028151298, 0.031009493, -0.012364317, -0.0017999177, -0.039802004, 0.024328217, 0.01271796, 0.039886657, 0.0074623595, 0.009858095, 0.057020597, -0.07718664, -0.0483008, -0.015155686, 0.04180382, 0.022811636, 0.00991879, 0.011291966, -0.02179413, -0.06996035, -0.0013573407, 0.01697971, -0.012854872, -0.007175533, 0.04548348, -0.0048131323, -0.0012508071, 0.03085887, -0.028651439, -0.042096313, 0.030586496, -0.0020689438, 0.013042054, -0.066238664, -0.01091073, 0.026564633, -0.013379165, 0.035066754, -0.04379128, -0.0002504738, 0.011266671, 0.0215735, -0.0073050815, 0.044369288, -0.036275443, -0.009644087, -0.023990814, 0.015584585, -0.02115572, 0.011938828, 0.020276986, 0.011093005, -0.04706234, 0.014505401, 0.038231723, 0.062233415, 0.02151975, -0.043097246, 0.034451317, 0.00644716, -0.06584034, -0.09072231, -0.066216536, -0.0050233956, 0.008993907, -0.02178335, 0.00965425, 0.012518615, -0.003592258, -0.021319337, 0.018747011, 0.038868506, -0.023259005, -0.00051984267, 0.021743618, 0.0058840555, -0.04202803, 0.036756698, 0.07017606, -0.0077401064, 0.041400734, 0.00057867187, -0.0704123, -0.017122721, 0.049036555, -0.007952814, 0.0103239175, -0.036126643, -0.02636954, 0.044193096, -0.015295757, 0.027914919, 0.06592626, -0.0010948431, -0.061605893, -0.045433067, -0.020212466, 0.038751796, -0.017580714, -0.004457368, -0.021242598, -0.0011174189, -0.0119580785, 0.0142066125, 0.0820738, -0.01514492, -0.0039108316, 0.07126742, 0.060911674, 0.0012008231, -0.010670029, -0.034728497, 0.01859967, 0.017157285, -0.026429802, -0.038137197, 0.08188021, 0.031816285, 0.023420116, -0.019889383, 0.0025028968, -0.0036825365, 0.021394981, -0.009683787, -0.041164216, 0.027273824, -0.07000919, 0.03795956, -0.05653687, -0.025284398, -0.016744927, 0.01827114, -0.038052462, -0.051757947, 0.035667304, -0.0077263047, 0.012657075, -0.07974694, -0.011306418, 0.11713082, 0.038858928, -0.022484468, -0.0125676105, 0.017585743, -0.005504003, 0.027505027, 0.021403037, 0.014869732, 0.035091583, -0.013319879, -0.0031904825, 0.00811912, 0.05396732, -0.037075773, 0.023085445, 0.07587209, 0.027228378, 0.049092427, 0.019926734, -0.026327321, 0.03878248, 0.019589933, -0.016979983, 0.03729434, 0.038305968, -0.0116716, -0.043691162, -0.0048422255, -0.00080547226, 0.004627596, 0.0020017265, -0.056377076, 0.06768244, -0.0536559, 0.004430995, 0.028574226, -0.018624058, 0.005428216, -0.04882717, -0.027371287, -0.002402283, -0.004539412, -0.031452812, -0.047010377, -0.049966756, 0.012235768, 0.017009893, -0.032374445, 0.00793228, -0.00220343, 0.030365728, -0.0033043572, 0.060746916, 0.03288227, -0.02590342, 0.025863552, 0.019887427, -0.018648855, -0.011363418, 0.05946576, 0.004167784, -0.007578468, -0.014738939, 0.012256558, -0.084281474, -0.00820497, 0.03086234, 0.010737953, 0.0015015929, -0.025964763, 0.03382323, 0.022740614, -0.013396271, -0.04297332, -0.00028926693, -0.033859346, -0.00069409463, -0.0007804434, 0.014026904, 0.021783346, 0.0051256493, -0.0021037995, -0.08614559, 0.007456209, -0.049341787, -0.016642628, -0.03994542, 0.024549602, 0.015970973, 0.043922532, 0.055958044, -0.022414416, 0.030048296, -0.045137964, -0.02815862, -0.0040756403, 0.05864846, -0.009532009, -0.025704669, 0.0047552884, -0.014303942, -0.045478635, -0.0051273652, 0.03675849, 0.0031358565, -0.0051031956, 0.028542735, -0.027987147, -0.0419716, 0.010922814, 0.008350255, 0.005230826, -0.035423424, 0.03252053, 0.0013745067, -0.021585135, 0.027317382, 0.03508564, -0.007901993, 0.06714316, -0.008934227, 0.0026415405, -0.039373588, 0.016485363, -0.04578956, 0.005067374, -0.0012580073, -0.054544162, 0.031627625, -0.030999364, -0.021667054, 0.013287015, -0.01950852, -0.011753219, -0.0069543035, 0.032345437, 0.0036158303, -0.013380872, 0.009736844, -0.015976481, 0.015763998, -0.022172779, -0.03696796, 0.012996864, 0.0452634, -0.030808117, -0.01865979, 0.035042807, -0.0010817159, -0.034854695, 0.0036234274, 0.017160187, 0.03315836, 0.017371697, 0.056296583, -0.030552236, -0.005571975, 0.02699559, -0.052761644, -0.025064534, -0.024628881, -0.012732814, -0.010898928, 0.03162439, -0.040458094, 0.042655084, -0.07255452, 2.5985093e-05, 0.0052705994, -0.0015083159, -0.06807616, -0.0436762, -0.00027870093, -0.014321804, 0.03134567, 0.051958516, -0.08975229, 0.014172449, -0.014797333, 0.011175973, 0.059194684, 0.045165006, 0.014474336, -0.014505811, -0.01353669, 0.05165326, -0.0028453406, -0.0033775985, -0.011520571, -0.014521399, 0.02447806, 0.046696108, 0.01977116, 0.0094583575, -0.093903996, 0.01105316, 0.0033503175, 0.0035670346, 0.05621031, -0.013291074, -0.039053004, 0.08251524, -0.025082158, -0.0067943605, -0.020376697, 0.03076387, -0.026383605, -0.06329241, -0.016902413, -0.030268254, -0.024791408, -0.0247664, 0.021397695, 0.0026529965, 0.012341567, 0.008795874, -0.016522363, -0.038351163, -0.008955185, -0.041113943, 0.03801726, -0.025830649, 0.0051191533, 0.0093088765, -0.040744506, 0.0337691, -0.02579837, 0.031785537, -0.023406008, 0.050263274, -0.039967895, 0.010605843, -0.025199337, -0.055409845, 0.05584957, 0.015911447]\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(embedding)\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"what is prompt engineering ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top scored chunks:\n",
      "\n",
      "Score: 0.889 | Passing: True\n",
      "Content: Prompt Engineering\n",
      "February 2025\n",
      "6\n",
      "Introduction\n",
      "When thinking about a large language model input and output, a text prompt (sometimes \n",
      "accompanied by other modalities such as image prompts) is the inp...\n",
      "\n",
      "Score: 0.836 | Passing: True\n",
      "Content: Prompt Engineering\n",
      "February 2025\n",
      "7\n",
      "When you chat with the Gemini chatbot,1 you basically write prompts, however this \n",
      "whitepaper focuses on writing prompts for the Gemini model within Vertex AI or by ...\n",
      "\n",
      "Score: 0.812 | Passing: True\n",
      "Content: Prompt Engineering\n",
      "February 2025\n",
      "13\n",
      "top-p settings. This can occur at both low and high temperature settings, though for different \n",
      "reasons. At low temperatures, the model becomes overly deterministic...\n",
      "\n",
      "Score: 0.778 | Passing: True\n",
      "Content: Prompt Engineering\n",
      "February 2025\n",
      "65\n",
      "We recommend creating a Google Sheet with Table 21 as a template. The advantages of \n",
      "this approach are that you have a complete record when you inevitably have to r...\n",
      "\n",
      "Score: 0.773 | Passing: True\n",
      "Content: Prompt Engineering\n",
      "February 2025\n",
      "67\n",
      "‚Ä¢ ReAct\n",
      "We even looked into ways how you can automate your prompts. \n",
      "The whitepaper then discusses the challenges of gen AI like the problems that can happen \n",
      "when ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "import asyncio\n",
    "# Set up evaluator\n",
    "evaluator = SemanticSimilarityEvaluator(\n",
    "    embed_model=embed_model,\n",
    "    similarity_threshold=0.75\n",
    ")\n",
    "# Reference is the HyDe-generated hypothetical doc\n",
    "reference = res\n",
    "async def score_and_sort_chunks(reference, retrieved_nodes):\n",
    "    scored = []\n",
    "    for node in retrieved_nodes:\n",
    "        response = node.get_content()\n",
    "        result = await evaluator.aevaluate(response=response, reference=reference)\n",
    "        scored.append((result.score, response, result.passing))\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    return scored\n",
    "# Run the scoring and sorting\n",
    "scored_chunks = await score_and_sort_chunks(reference, retriever.retrieve(query))\n",
    "print(\"\\n Top scored chunks:\")\n",
    "for score, content, passing in scored_chunks[:5]:\n",
    "    print(f\"\\nScore: {score:.3f} | Passing: {passing}\\nContent: {content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Final Answer:\n",
      " Prompt engineering is the process of designing high-quality prompts that guide large language models (LLMs) to produce accurate outputs. This process involves tinkering to find the best prompt, optimizing prompt length, and evaluating a prompt‚Äôs writing style and structure in relation to the task. It is an iterative process, as inadequate prompts can lead to ambiguous or inaccurate responses.\n"
     ]
    }
   ],
   "source": [
    "top_chunks = [chunk[1] for chunk in scored_chunks[:3]]  # top 3 contents\n",
    "context = \"\\n\\n\".join(top_chunks)\n",
    "final_prompt = f\"\"\"\n",
    "Use the following context to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(final_prompt)\n",
    "print(\" Final Answer:\\n\", response.content.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def HydeRAG(query):\n",
    " is_query_related_to_document= vector_index.as_query_engine().query(f\"Is the following query: {query} related to this document ?\")\n",
    " #  print(is_query_related_to_document)\n",
    " #  print(type(is_query_related_to_document))\n",
    " if(\"no\" in str(is_query_related_to_document).lower()):\n",
    "   return str(is_query_related_to_document)\n",
    " else:\n",
    " #  res=generate_hypothetical_answer(query,llm)\n",
    "  print(res)\n",
    "  print(\" \")\n",
    "  #  Embed HyDe-generated hypothetical answer\n",
    "  res_embedding = embed_model.get_text_embedding(res)\n",
    "  #  Retrieve chunks using the original query (you can use hypo-based retrieval too)\n",
    "  _nodes = retriever.retrieve(query)\n",
    "  #  Score by dot product\n",
    "  scored_nodes = []\n",
    "  for node in _nodes:\n",
    "    chunk_embedding = embed_model.get_text_embedding(node.get_content())  # force embed\n",
    "    dot_score = np.dot(res_embedding, chunk_embedding) / (\n",
    "    np.linalg.norm(res_embedding) * np.linalg.norm(chunk_embedding))\n",
    "    scored_nodes.append((dot_score, node))\n",
    "  #  Sort and pick top-k\n",
    "  scored_nodes.sort(reverse=True, key=lambda x: x[0])\n",
    "  # print(score_nodes)\n",
    "  top_k_nodes = [node for _, node in scored_nodes[:5]]\n",
    "  #  Final generation\n",
    "  context = \"\\n\\n\".join([node.get_content() for node in top_k_nodes])\n",
    "  final_prompt = f\"\"\"\n",
    "  Use the following context to answer the user's question.\n",
    "\n",
    "  Context:\n",
    "  {context}\n",
    "\n",
    "  Question: {query}\n",
    "\n",
    "  Answer:\n",
    "  \"\"\"\n",
    "\n",
    "  response = llm.invoke(final_prompt)\n",
    "  print(\" Final Answer:\\n\", response.content.strip())\n",
    "  return response.content.strip()\n",
    "# query=\"What is zero-shot prompting and when should I use it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Final Answer:\n",
      " Prompt engineering is the process of designing high-quality prompts that guide Large Language Models (LLMs) to produce accurate outputs. This process involves tinkering to find the best prompt, optimizing prompt length, and evaluating a prompt‚Äôs writing style and structure in relation to the task. It is an iterative process of crafting, testing, analyzing, documenting, and refining prompts based on the model‚Äôs performance.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG(query) #query= \"what is prompt engineering ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical Answer:\n",
      "Contextual prompting significantly improves model responses by providing the AI with specific background information, examples, or constraints directly within the prompt. This additional context helps reduce ambiguity, guides the model towards the desired scope and tone, and supplies it with relevant knowledge it might not otherwise access or prioritize. Consequently, the model can generate more accurate, relevant, and coherent outputs that directly address the user's intent, moving beyond generic or potentially incorrect responses.\n",
      " \n",
      " Final Answer:\n",
      " Contextual prompting helps improve model responses by:\n",
      "\n",
      "*   Providing specific details or background information relevant to the current conversation or task.\n",
      "*   Helping the model understand the nuances of what's being asked.\n",
      "*   Enabling the model to tailor its response accordingly.\n",
      "*   Allowing the model to more quickly understand the request.\n",
      "*   Leading to the generation of more accurate and relevant responses.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG(\" How does contextual prompting help improve model responses?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_avg_embedding_from_multiple_hypotheticals(query, llm, embed_model, n=3):\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(n):\n",
    "        prompt = f\"\"\"\n",
    "You are a helpful assistant. Generate a short paragraph that might answer the user's question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Hypothetical Answer{i+1}:\n",
    "\"\"\"\n",
    "        response = llm.invoke(prompt)\n",
    "        hypo_doc = response.content.strip()\n",
    "\n",
    "        # Embed each hypothetical document\n",
    "        emb = embed_model.get_text_embedding(hypo_doc)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    # Average all embeddings\n",
    "    avg_embedding = [sum(x)/n for x in zip(*embeddings)]\n",
    "\n",
    "    return avg_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def HydeRAG_Multiple(query):\n",
    " print(\" \")\n",
    " is_query_related_to_document= vector_index.as_query_engine().query(f\"Is the following query: {query} related to this document ?\")\n",
    " #  print(is_query_related_to_document)\n",
    " #  print(type(is_query_related_to_document))\n",
    " #  print(str(is_query_related_to_document).lower())\n",
    " if(\"no\" in str(is_query_related_to_document).lower()):\n",
    "   return str(is_query_related_to_document)\n",
    " # Embed HyDe-generated hypothetical answer\n",
    " res_embedding =generate_avg_embedding_from_multiple_hypotheticals(query,llm,embed_model,n=3)\n",
    "#  print(res_embedding)\n",
    " #  Retrieve chunks using the original query\n",
    " _nodes = retriever.retrieve(query)\n",
    " #  Score by cosine similarity\n",
    " scored_nodes = []\n",
    " for node in _nodes:\n",
    "    chunk_embedding = embed_model.get_text_embedding(node.get_content())  # force embed\n",
    "    dot_score = np.dot(res_embedding, chunk_embedding) / (\n",
    "    np.linalg.norm(res_embedding) * np.linalg.norm(chunk_embedding))\n",
    "    scored_nodes.append((dot_score, node))\n",
    "\n",
    " # Sort and pick top-k\n",
    " scored_nodes.sort(reverse=True, key=lambda x: x[0])\n",
    " top_k_nodes = [node for _, node in scored_nodes[:5]]\n",
    "\n",
    " # Final generation\n",
    " context = \"\\n\\n\".join([node.get_content() for node in top_k_nodes])\n",
    " final_prompt = f\"\"\"\n",
    " Use the following context to answer the user's question.\n",
    "\n",
    " Context:\n",
    " {context}\n",
    "\n",
    " Question: {query}\n",
    "\n",
    " Answer:\n",
    " \"\"\"\n",
    "\n",
    " response = llm.invoke(final_prompt)\n",
    " print(\" Final Answer:\\n\", response.content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Final Answer:\n",
      " ReAct (reason & act) prompting is a paradigm that enables Large Language Models (LLMs) to solve complex tasks by combining natural language reasoning with the use of external tools (such as search or code interpreters). This allows the LLM to perform actions like interacting with external APIs to retrieve information, which is a step towards agent modeling.\n",
      "\n",
      "ReAct combines reasoning and acting into a thought-action loop. This process works as follows:\n",
      "1.  The LLM first reasons about the problem and generates a plan of action.\n",
      "2.  It then performs the actions outlined in the plan and observes the results.\n",
      "3.  The LLM uses these observations to update its reasoning and generate a new plan of action.\n",
      "This thought-action loop continues until the LLM reaches a solution to the problem. This approach mimics how humans operate by reasoning verbally and taking actions to gain information.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG_Multiple(\"What is ReAct prompting and how does it combine reasoning and action?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReAct (Reasoning and Acting) prompting is a technique that enables large language models (LLMs) to interleave verbal reasoning traces with specific actions. It combines the LLM's ability to generate internal thoughts, plans, and reflections (reasoning) with its capacity to interact with external tools or environments (action). The model first \"reasons\" about the problem, formulating a plan or a step, then \"acts\" by executing a tool (like a search engine or calculator) based on that reasoning. It then observes the tool's output and \"reasons\" again about the next step, creating a dynamic, iterative loop that allows for more complex problem-solving, self-correction, and improved performance on tasks requiring multi-step operations.\n",
      " \n",
      " Final Answer:\n",
      " ReAct (reason & act) prompting is a paradigm that enables Large Language Models (LLMs) to solve complex tasks. It achieves this by combining natural language reasoning with the use of external tools (such as search or code interpreters) to perform actions, like interacting with external APIs to retrieve information. This approach is considered a first step towards agent modeling and mimics how humans operate by reasoning verbally and taking actions to gain information.\n",
      "\n",
      "ReAct combines reasoning and acting through a \"thought-action loop.\" The process works as follows:\n",
      "1. The LLM first reasons about the problem and generates a plan of action.\n",
      "2. It then performs the actions specified in the plan and observes the results.\n",
      "3. The LLM uses these observations to update its reasoning.\n",
      "4. Based on the updated reasoning, it generates a new plan of action.\n",
      "This iterative process continues until the LLM reaches a solution to the problem.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG(\"What is ReAct prompting and how does it combine reasoning and action?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Final Answer:\n",
      " Even for complex tasks, the prompt itself should be designed with simplicity, clarity, and conciseness. Avoid complex language and unnecessary information.\n",
      "\n",
      "Here's how you should design a prompt for complex tasks:\n",
      "\n",
      "1.  **Provide Examples:** This is highlighted as the most important best practice. For complex tasks, provide one-shot or few-shot examples within the prompt. This acts as a powerful teaching tool, showcasing desired outputs or similar responses, allowing the model to learn and tailor its generation. Ensure your examples are:\n",
      "    *   Relevant to the task.\n",
      "    *   Diverse.\n",
      "    *   High quality and well-written.\n",
      "    *   Include edge cases if you need robust output for varied inputs.\n",
      "\n",
      "2.  **Be Specific about the Output:** A concise instruction might not be enough for complex tasks. Provide specific details about the desired format, style, or content of the response. This helps the model focus on what's relevant and improves accuracy.\n",
      "\n",
      "3.  **Use Instructions over Constraints:** Guide the model on what it *should do* or *produce* (instructions) rather than just what it *should not do* (constraints).\n",
      "\n",
      "4.  **Use Action Verbs:** Clearly define the action you want the model to perform by using strong verbs such as Act, Analyze, Categorize, Classify, Compare, Create, Describe, Define, Evaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, Predict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, Summarize, Translate, or Write.\n",
      "\n",
      "Remember that prompt engineering is an iterative process, so finding the right prompt for complex tasks may require tinkering and refinement.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG_Multiple(\"How should I design a prompt for complex tasks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothetical Answer:\n",
      "For complex tasks, design your prompt with clarity, structure, and specificity. Begin by clearly defining the ultimate goal and providing all necessary context or background information. Break the task down into smaller, manageable steps or sub-tasks, outlining the desired process or logic for each. Crucially, specify the exact output format you expect, including any constraints, and consider providing examples of what good output looks like to minimize ambiguity and guide the AI effectively.\n",
      " \n",
      " Final Answer:\n",
      " For complex tasks, the context suggests designing prompts using the following strategies:\n",
      "\n",
      "1.  **Provide Examples:** This is highlighted as the \"most important best practice.\" For complex tasks, providing one-shot or few-shot examples within the prompt is highly effective. These examples act as a powerful teaching tool, showcasing desired outputs and helping the model learn and tailor its generation. Ensure your examples are relevant, diverse, high-quality, well-written, and include edge cases to make the output robust.\n",
      "\n",
      "2.  **Be Specific About the Output:** A concise or generic instruction will not guide the LLM enough for complex tasks. Provide specific details in the prompt (through system or context prompting) to help the model focus on what‚Äôs relevant and improve accuracy.\n",
      "\n",
      "3.  **Design with Simplicity:** Even for complex tasks, the prompt itself should be concise, clear, and easy to understand. Avoid complex language and unnecessary information. Using action verbs (e.g., Act, Analyze, Create, Describe, Generate, Summarize) can help make instructions clearer.\n",
      "\n",
      "4.  **Use Instructions over Constraints:** Guide the model explicitly on what it *should do* or produce, rather than just what it *should not do*.\n",
      "\n",
      "5.  **Embrace an Iterative Process:** Crafting the most effective prompt, especially for complex tasks, is an iterative process. You may need to tinker and refine your prompt to achieve the desired output, as inadequate prompts can lead to ambiguous or inaccurate responses.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG(\"How should I design a prompt for complex tasks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Final Answer:\n",
      " Based on the provided context, prompts interact with the model's internal behavior in several ways:\n",
      "\n",
      "1.  **Input for Prediction:** A text prompt is the input the model uses to predict a specific output. The clearer the prompt text, the better the LLM can predict the next likely text.\n",
      "2.  **Influence on Efficacy:** Many aspects of a prompt, including word-choice, style, tone, structure, and context, affect the model's efficacy and its ability to provide meaningful output. Inadequate prompts can lead to ambiguous or inaccurate responses.\n",
      "3.  **Guidance for Generation:** LLMs are tuned to follow instructions. Prompts guide how LLMs generate text:\n",
      "    *   **System prompting** sets the overall context and purpose, defining the model's fundamental capabilities and overarching purpose. It can also specify how to return the output (e.g., format, structure).\n",
      "    *   **Contextual prompting** provides specific details or background information, helping the model understand nuances and tailor responses accordingly. It provides immediate, task-specific information.\n",
      "    *   **Role prompting** assigns a specific character or identity, helping the model generate responses consistent with that role's knowledge, behavior, style, and voice.\n",
      "4.  **Impact on Sampling Process:** Prompt engineering, implicitly through model configurations like temperature and top-k/top-p values, affects the model's internal sampling process, influencing the balance between determinism and randomness in its output.\n",
      "5.  **Focus and Accuracy:** Providing specific details in the prompt (through system or context prompting) helps the model focus on what's relevant, improving overall accuracy.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG_Multiple(\"How do prompts interact with the model‚Äôs internal behavior?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Final Answer:\n",
      " According to the context, prompting for a JSON format can help limit hallucinations by forcing the model to create a structure.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG_Multiple(\"How can prompts help reduce hallucinations?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make your prompts more reliable, focus on clarity and specificity: clearly define the AI's role, provide concrete examples, and specify the desired output format. Incorporate constraints and guardrails to limit scope and prevent unwanted responses. Finally, iterate and test your prompts rigorously, refining them based on the consistency and quality of the AI's outputs.\n",
      " \n",
      " Final Answer:\n",
      " Based on the provided context, here's how to make your prompts more reliable:\n",
      "\n",
      "1.  **Be Specific about the Output:** Provide specific details in your prompt (through system or context prompting) to help the model focus on what's relevant, improving overall accuracy. A concise or generic instruction might not guide the LLM enough. (Page 56)\n",
      "2.  **Use Instructions over Constraints:** Guide the model on what it *should* do or produce (instructions) rather than just what it *should not* do (constraints). (Page 56)\n",
      "3.  **Make Prompt Text Clearer:** The clearer your prompt text, the better it is for the LLM to predict the next likely text and generate a relevant answer. (Page 13)\n",
      "4.  **Iterate and Experiment:** Prompt engineering is an iterative process. Craft and test different prompts, analyze, and document the results. Refine your prompt based on the model‚Äôs performance and keep experimenting until you achieve the desired output. (Page 6, 65)\n",
      "5.  **Document Prompt Attempts:** Document your prompt attempts in full detail, including the version, whether the result was OK/NOT OK/SOMETIMES OK, and feedback. This helps you learn over time what works and what doesn't, and allows you to revisit your work, test performance on different model versions, and debug errors. (Page 64, 65)\n",
      "6.  **Use Automated Tests and Evaluation:** Ideally, your prompts should be part of an operationalized system, and you should rely on automated tests and evaluation procedures to understand how well your prompt generalizes to a task. (Page 65)\n",
      "7.  **Adjust Model Configurations (e.g., Temperature for CoT):** For specific techniques like Chain of Thought (CoT) prompting, setting the temperature to 0 is recommended to achieve a single correct answer, as reasoning often leads to one correct outcome. (Page 64)\n"
     ]
    }
   ],
   "source": [
    "HydeRAG(\"How do I make my prompts more reliable?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Final Answer:\n",
      " A system prompt sets the overall context and purpose for the language model, defining its fundamental capabilities and overarching purpose. It provides additional tasks or instructions to the system on how to return the output.\n",
      "\n",
      "You should include explicit instructions on the desired format, style, or content of the response. Examples of what to include are:\n",
      "\n",
      "*   **How to return the output:** For instance, specifying to \"Only return the label in uppercase\" or to \"return the output in JSON format.\"\n",
      "*   **Specific requirements for the output:** Such as generating a code snippet compatible with a specific programming language, or returning a certain structure.\n",
      "*   **Instructions for safety and toxicity control:** For example, adding a line like \"You should be respectful in your answer.\"\n",
      "*   **Specific details** to help the model focus on what's relevant and improve accuracy.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG_Multiple(\"What should I include in a system prompt?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'No, the query \"what is meditation?\" is not related to this document. The document\\'s content pertains to prompt engineering, including topics like JSON repair, working with schemas, and best practices for prompt attempts.'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HydeRAG(\"what is meditation ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To ensure the model stays focused, craft your prompts with clarity and specificity, outlining the exact task, desired format, and any constraints like length or tone. Providing relevant context upfront can also prevent tangents. If the model begins to stray, gently redirect it with follow-up prompts that reiterate the core objective or correct its course, reinforcing the boundaries of the task.\n",
      " \n",
      " Final Answer:\n",
      " To make sure the model stays focused, you should:\n",
      "\n",
      "1.  **Be specific about the desired output:** Provide specific details in your prompt (through system or context prompting) to help the model focus on what's relevant and improve overall accuracy. A concise or generic instruction might not guide the LLM enough.\n",
      "    *   **DO:** \"Generate a 3 paragraph blog post about the top 5 video game consoles. The blog post should be informative and engaging, and it should be written in a conversational style.\"\n",
      "    *   **DO NOT:** \"Generate a blog post about video game consoles.\"\n",
      "\n",
      "2.  **Use clearer prompt text:** The clearer your prompt text, the better it is for the LLM to predict the next likely text and generate a relevant answer.\n",
      "\n",
      "3.  **Utilize specific prompting techniques:** Techniques like \"step-back prompting\" can increase the accuracy of your prompts by first asking the model to generate foundational information and then using that as context for the main task. This helps the model stay on track and produce more relevant results.\n",
      "\n",
      "4.  **Adjust temperature and top-k/top-p settings:** Careful tinkering with these values can help find the optimal balance between determinism and randomness, preventing the model from becoming overly deterministic (sticking rigidly to one path, leading to loops) or excessively random (increasing the probability of returning to a prior state, also leading to loops). This prevents the model's sampling process from getting \"stuck\" and producing monotonous or unhelpful output.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG(\"How do I make sure the model stays focused?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Final Answer:\n",
      " Even for complex tasks, the prompt itself should be designed with simplicity, clarity, and conciseness. Avoid complex language and unnecessary information.\n",
      "\n",
      "Here's how you should design a prompt for complex tasks:\n",
      "\n",
      "1.  **Provide Examples:** This is highlighted as the most important best practice. For complex tasks, provide one-shot or few-shot examples within the prompt. This acts as a powerful teaching tool, showcasing desired outputs or similar responses, allowing the model to learn and tailor its generation. Ensure your examples are:\n",
      "    *   Relevant to the task.\n",
      "    *   Diverse.\n",
      "    *   High quality and well-written.\n",
      "    *   Include edge cases if you need robust output for varied inputs.\n",
      "\n",
      "2.  **Be Specific about the Output:** A concise instruction might not be enough for complex tasks. Provide specific details about the desired format, style, or content of the response. This helps the model focus on what's relevant and improves accuracy.\n",
      "\n",
      "3.  **Use Instructions over Constraints:** Guide the model on what it *should do* or *produce* (instructions) rather than just what it *should not do* (constraints).\n",
      "\n",
      "4.  **Use Action Verbs:** Clearly define the action you want the model to perform by using strong verbs such as Act, Analyze, Categorize, Classify, Compare, Create, Describe, Define, Evaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, Predict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, Summarize, Translate, or Write.\n",
      "\n",
      "Remember that prompt engineering is an iterative process, so finding the right prompt for complex tasks may require tinkering and refinement.\n"
     ]
    }
   ],
   "source": [
    "HydeRAG_Multiple(\"How should I design a prompt for complex tasks?\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
