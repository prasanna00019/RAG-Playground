{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Hypothetical Prompt Embedding RAG (HyPE) - Q2Q Retrieval\n",
    "\n",
    "Welcome to the **Hypothetical Prompt Embedding RAG** project!  \n",
    "This notebook demonstrates an advanced Retrieval-Augmented Generation (RAG) workflow using **Hypothetical Prompt Embeddings (HyPE)**, inspired by recent research.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 What is HyPE?\n",
    "\n",
    "Traditional RAG systems retrieve document chunks based on query-to-document similarity.  \n",
    "**HyPE** takes a novel approach:  \n",
    "- For each document chunk, it generates a set of *hypothetical questions* that the chunk could answer.\n",
    "- At query time, the user’s question is embedded and matched **directly to these hypothetical questions** (Q2Q matching), rather than to the original document text.\n",
    "\n",
    "This leads to more precise and contextually relevant retrieval, bridging the gap between user queries and document content.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ How It Works\n",
    "\n",
    "1. **Document Chunking:**  \n",
    "    Documents are split into manageable chunks.\n",
    "\n",
    "2. **Hypothetical Question Generation:**  \n",
    "    For each chunk, an LLM generates essential questions that the chunk could answer.\n",
    "\n",
    "3. **Embedding:**  \n",
    "    Both hypothetical questions and user queries are embedded using a powerful embedding model.\n",
    "\n",
    "4. **Q2Q Retrieval:**  \n",
    "    At query time, the user’s question is matched against the pool of hypothetical questions using vector similarity (cosine/FAISS).\n",
    "\n",
    "5. **Answer Generation:**  \n",
    "    The most relevant chunks are used as context for the LLM to generate a final answer.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Features\n",
    "\n",
    "- End-to-end HyPE pipeline implemented from scratch\n",
    "- Uses Google Gemini and Google Embeddings\n",
    "- Supports both brute-force and FAISS-based retrieval\n",
    "- Modular and extensible code\n",
    "\n",
    "---\n",
    "## CITATION \n",
    "``` \n",
    "Vake, Domen and Vičič, Jernej and Tošić, Aleksandar, Bridging the Question-Answer Gap in Retrieval-Augmented Generation: Hypothetical Prompt Embeddings. Available at SSRN: https://ssrn.com/abstract=5139335 or http://dx.doi.org/10.2139/ssrn.5139335\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 References\n",
    "\n",
    "- **Bridging the Question-Answer Gap in Retrieval-Augmented Generation: Hypothetical Prompt Embeddings**  \n",
    "  [Research Paper Link](https://www.researchgate.net/publication/389032824_Bridging_the_Question-Answer_Gap_in_Retrieval-Augmented_Generation_Hypothetical_Prompt_Embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Credits\n",
    "\n",
    "This project is implemented from scratch, inspired by the above paper.  \n",
    "All code and logic are original and tailored for educational and research purposes.\n",
    "\n",
    "---\n",
    "\n",
    "Happy experimenting! 🤖✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HyPE (Hypothetical Prompt Embeddings) is an advanced RAG enhancement technique that\n",
    "precomputes hypothetical questions for each document chunk during indexing rather\n",
    "than generating content at query time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GOOGLE_API_KEY = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "splitter = SentenceSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "embed_model = GoogleGenAIEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    embed_batch_size=100,\n",
    "    api_key=\"\"\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_embeddings(nodes, llm, embed_model):\n",
    "    \"\"\"\n",
    "    For each node, generate hypothetical questions and embed them,\n",
    "    associating each embedding with the original node.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: (question_embedding, original_node)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\n",
    "    Analyze the input text and generate essential questions that, when answered, capture\n",
    "    the main points and core meaning of the text. The questions should be exhaustive and\n",
    "    understandable without context. When possible, named entities should be referenced\n",
    "    by their full name. Only answer with questions where each question should be written\n",
    "    in its own line (separated by newline) with no prefix.\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_pairs = []\n",
    "\n",
    "    for node in nodes:\n",
    "        #  Generate questions from node text\n",
    "        full_prompt = prompt + \"\\n\\n\" + node.text\n",
    "        response = llm.invoke(full_prompt)\n",
    "\n",
    "        #  Split response into individual questions\n",
    "        questions = [q.strip() for q in response.content.split('\\n') if q.strip()]\n",
    "\n",
    "        #  Get embeddings for all questions in batch\n",
    "        question_embeddings = embed_model.get_text_embedding_batch(questions)\n",
    "\n",
    "        #  Associate each embedding with the original node\n",
    "        for embedding in question_embeddings:\n",
    "            embedding_pairs.append((embedding, node))\n",
    "\n",
    "    return embedding_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pairs=generate_hypothetical_embeddings(nodes,llm,embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement Learning\n",
      "● Learn in an interactive environment by Trial-and-Error using Feedback \n",
      "(reward/penalty) for actions\n",
      "○ In supervised learning there is feedback- correct / incorrect, HERE IT IS PENALTY/REWARD\n",
      "○ In unsupervised learning the similarity/difference among data items found,HERE THE AIM IS \n",
      "TO LEARN POLICY / ACTION MODEL THAT MAXIMIZE REWARD\n"
     ]
    }
   ],
   "source": [
    "print(embedding_pairs[54][1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_user=\"Is there a technique which is in between supervised and unsupervised learning ? explain it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embed = embed_model.get_text_embedding(q_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "#  E = list of (question_embedding, node)\n",
    "def retrieve_top_chunks(q_embed, E, top_k=5):\n",
    "    similarities = []\n",
    "\n",
    "    for emb, node in E:\n",
    "        sim = cosine_similarity(\n",
    "            np.array(q_embed).reshape(1, -1),\n",
    "            np.array(emb).reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities.append((sim, node))\n",
    "\n",
    "    # Sort by similarity score\n",
    "    similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    # Return top-k unique nodes (chunks)\n",
    "    seen = set()\n",
    "    top_nodes = []\n",
    "    for sim, node in similarities:\n",
    "        if node.node_id not in seen:\n",
    "            top_nodes.append((sim, node))\n",
    "            seen.add(node.node_id)\n",
    "        if len(top_nodes) >= top_k:\n",
    "            break\n",
    "\n",
    "    return top_nodes\n",
    "top_nodes=retrieve_top_chunks(q_embed,embedding_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there is a technique called **Semi-supervised Learning** which is in between supervised and unsupervised learning.\n",
      "\n",
      "**Explanation:**\n",
      "Semi-supervised learning utilizes a small portion of labeled data along with a large amount of unlabeled data. It works by:\n",
      "1.  Using the small portion of labeled data to learn a partial model.\n",
      "2.  Using this partial model to generate \"pseudo-labels\" for the unlabeled data.\n",
      "3.  Combining both the original labeled data and the newly pseudo-labeled data.\n",
      "4.  Finally, learning a complete model from this combined dataset to make predictions for new examples.\n",
      "\n",
      "This approach bridges the gap between supervised learning (which relies entirely on labeled data) and unsupervised learning (which uses only unlabeled data to find patterns).\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(query, top_nodes, llm):\n",
    "    context = \"\\n\\n\".join([node.text for _, node in top_nodes])\n",
    "    prompt = f\"\"\"Answer the following question based on the provided context.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "final_answer=generate_answer(q_user,top_nodes,llm)\n",
    "print(final_answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question_embeddings = []\n",
    "metadata = []\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(nodes, llm, embed_model):\n",
    "    index = faiss.IndexFlatL2(768)\n",
    "\n",
    "    for node in nodes:\n",
    "        # 1. Generate questions\n",
    "        prompt = \"\"\"Analyze the input text and generate essential questions that, when answered, capture\n",
    "        the main points and core meaning of the text. Questions should be exhaustive and written line-by-line with no prefix.\\n\\n\"\"\" + node.text\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        questions = [q.strip() for q in response.content.split(\"\\n\") if q.strip()]\n",
    "        if not questions:\n",
    "            continue\n",
    "\n",
    "        # 2. Get embeddings\n",
    "        embeddings = embed_model.get_text_embedding_batch(questions)\n",
    "        embeddings = [np.array(e).astype(\"float32\") for e in embeddings]\n",
    "\n",
    "        # 3. Normalize for cosine similarity\n",
    "        for emb in embeddings:\n",
    "            norm = np.linalg.norm(emb)\n",
    "            if norm > 0:\n",
    "                emb /= norm\n",
    "            question_embeddings.append(emb)\n",
    "            metadata.append((node.text, node.node_id))\n",
    "\n",
    "    # Convert to FAISS format\n",
    "    index.add(np.array(question_embeddings))\n",
    "    return index, metadata\n",
    "\n",
    "faiss_index, metadata = build_faiss_index(nodes, llm, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(query: str, faiss_index, metadata, embed_model, top_k=5):\n",
    "    # embed the query\n",
    "    q_vec = embed_model.get_text_embedding(query)\n",
    "    q_vec = np.array(q_vec).astype(\"float32\")\n",
    "\n",
    "    # normalize for cosine similarity\n",
    "    norm = np.linalg.norm(q_vec)\n",
    "    if norm > 0:\n",
    "        q_vec /= norm\n",
    "\n",
    "    # Search FAISS\n",
    "    _, I = faiss_index.search(np.array([q_vec]), top_k)\n",
    "\n",
    "    # Retrieve corresponding chunks\n",
    "    seen_ids = set()\n",
    "    top_chunks = []\n",
    "    for idx in I[0]:\n",
    "        if idx < len(metadata):\n",
    "            chunk_text, node_id = metadata[idx]\n",
    "            if node_id not in seen_ids:\n",
    "                top_chunks.append(chunk_text)\n",
    "                seen_ids.add(node_id)\n",
    "    return top_chunks\n",
    "\n",
    "def generate_final_answer(query: str, retrieved_chunks, llm):\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"\"\"\n",
    "    You are an intelligent assistant. Use the following context to answer the user's question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query):\n",
    "    retrieved_chunks = retrieve_chunks(query, faiss_index, metadata, embed_model, top_k=5)\n",
    "    answer = generate_final_answer(query, retrieved_chunks, llm)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there is a technique called **Semi-supervised Learning** which is in between supervised and unsupervised learning.\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "*   **Semi-supervised Learning** uses a small portion of **Labeled Data** and a large number of **Unlabeled Data**.\n",
      "*   It then learns a model and makes a prediction for a new example.\n",
      "\n",
      "This makes it \"in between\" because:\n",
      "*   **Supervised Learning** primarily uses **Labeled Data**.\n",
      "*   **Unsupervised Learning** primarily uses **UNLABELED Data**.\n",
      "*   **Semi-supervised Learning** leverages both, combining the benefits of having some labeled examples with the abundance of unlabeled data.\n"
     ]
    }
   ],
   "source": [
    "query = \"Is there a technique which is in between supervised and unsupervised learning ? explain it\"\n",
    "print(answer_query(query).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning (ML) algorithms learn by building mathematical models from sample data and identifying patterns within that data. This process allows computers to learn without being explicitly programmed for every possible scenario. By using data and algorithms, ML aims to imitate the way humans learn, gradually improving its accuracy over time.\n"
     ]
    }
   ],
   "source": [
    "query_2=\"How do machine learning algorithms learn from data? explain that\"\n",
    "print(answer_query(query_2).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computers learn from data by using **algorithms** to imitate the way humans learn. Specifically, **Machine Learning (ML) algorithms** learn mathematical models from sample data and patterns, gradually improving their accuracy over time. This process gives computers the ability to learn without being explicitly programmed.\n",
      "\n",
      "This technique of learning from data is called **Machine Learning**.\n",
      "\n",
      "**Machine Learning** is a branch of Computer Science and Artificial Intelligence that focuses on enabling computers to learn from data. It involves using data and algorithms to build mathematical models that can identify patterns and make predictions or decisions. Machine Learning is generally used for tasks such as prediction, classification, and clustering. There are various types of Machine Learning, including Supervised Learning, Unsupervised Learning, Reinforcement Learning, and Semi-supervised Learning.\n"
     ]
    }
   ],
   "source": [
    "query_3=\"How do computers learn from data ? what is this technique of learning from data called ? explain that technique\"\n",
    "print(answer_query(query_3).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but the provided context does not contain information about \"data science.\" It focuses on defining and explaining \"Machine Learning.\"\n"
     ]
    }
   ],
   "source": [
    "query_4=\"what is data science ?\"\n",
    "print(answer_query(query_4).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement Learning works as follows:\n",
      "\n",
      "1.  **Interactive Environment:** The learning process takes place within an interactive environment.\n",
      "2.  **Trial-and-Error:** The learning agent explores this environment by performing actions through a process of trial-and-error.\n",
      "3.  **Feedback (Reward/Penalty):** For each action taken, the agent receives feedback in the form of a reward or a penalty. This type of feedback is distinct from the \"correct/incorrect\" labels found in supervised learning.\n",
      "4.  **Goal:** The primary objective is to learn a \"policy\" or \"action model\" that dictates which actions to take in different situations, with the ultimate aim of maximizing the accumulated reward over time.\n"
     ]
    }
   ],
   "source": [
    "query_5=\"Give a breakdown of the working of reinforcement learning \"\n",
    "print(answer_query(query_5).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machines find patterns in data by using **Machine Learning (ML) algorithms** that learn **mathematical models** from sample data. These algorithms are designed to identify relationships, structures, or regularities within the data, gradually improving their accuracy without being explicitly programmed for every specific pattern.\n",
      "\n",
      "The kind of ML technique that focuses on finding patterns in data, especially when the data is not labeled, is **Unsupervised Learning**. A common application of Unsupervised Learning for finding patterns is **Clustering**.\n",
      "\n",
      "**Example:**\n",
      "Imagine a large dataset of customer purchasing behavior, including items bought, frequency of purchases, and total spending. A machine learning algorithm using **Clustering** (an Unsupervised Learning technique) can analyze this data to find natural groupings or patterns among customers.\n",
      "\n",
      "For instance, it might identify:\n",
      "*   A cluster of \"high-value shoppers\" who buy frequently and spend a lot.\n",
      "*   A cluster of \"bargain hunters\" who only buy during sales.\n",
      "*   A cluster of \"occasional buyers\" who purchase only a few specific items once in a while.\n",
      "\n",
      "The machine was not told beforehand what these groups are; it *discovered* these patterns and segments based on the inherent similarities in the data, allowing a business to understand its customer base better without prior definitions.\n"
     ]
    }
   ],
   "source": [
    "query_6=\"how do machines find patterns in data ? what kind of ML technique is this ?explain with a example\"\n",
    "print(answer_query(query_6).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machines learn the relationship between a Dependent variable (target) and a set of independent variables (predictors) through **Regression**. This technique falls under **Supervised Learning**.\n"
     ]
    }
   ],
   "source": [
    "query_7=\"how do machines learn relationship between a Dependent variable and a set of independent variables ? which class of technique is this ? \"\n",
    "print(answer_query(query_7).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The way machines can learn by giving them rewards, similar to how children learn, is through a branch of Machine Learning called **Reinforcement Learning**.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1.  **Interactive Environment:** The machine operates within an interactive environment.\n",
      "2.  **Trial-and-Error:** It learns by performing actions through a process of trial-and-error.\n",
      "3.  **Feedback (Reward/Penalty):** For each action it takes, the machine receives immediate feedback in the form of a **reward** (if the action was good or moved it closer to its goal) or a **penalty** (if the action was bad or moved it away from its goal). This is explicitly stated as being different from the \"correct/incorrect\" feedback in supervised learning.\n",
      "4.  **Learning a Policy:** The machine uses this reward/penalty feedback to gradually learn a \"policy\" or \"action model.\" This model dictates which actions it should take in different situations.\n",
      "5.  **Maximizing Reward:** The ultimate aim of this learning process is to learn a policy that will **maximize the total reward** it receives over time. Just like a child learns to repeat actions that earn praise or treats and avoid actions that lead to scolding, the machine learns to favor actions that lead to higher rewards.\n"
     ]
    }
   ],
   "source": [
    "query_8=\"how can we make machines learn by giving them reward as we give to children for doing the right things? explain that\"\n",
    "print(answer_query(query_8).content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
