{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Dynamic Prompt-Aware RAG System using LlamaIndex\n",
        "\n",
        "This project showcases a Retrieval-Augmented Generation (RAG) pipeline built with [LlamaIndex](https://www.llamaindex.ai/), enhanced with **dynamic prompt routing based on query tone or intent**. It demonstrates how a simple RAG setup can be evolved into an intelligent, user-aware system that responds in varying tones such as comparative, creative, elaborative, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What is this project?\n",
        "\n",
        "A document-based Q&A system that:\n",
        "- Loads and indexes PDFs using embeddings\n",
        "- Accepts user queries\n",
        "- Dynamically detects the *tone* or *intent* of each query\n",
        "- Routes it to an appropriate **custom prompt template**\n",
        "- Performs RAG using the selected prompt and LLM\n",
        "- Returns intelligent, context-aware answers\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Project Evolution\n",
        "\n",
        "### ‚úÖ Phase 1: Basic RAG with LlamaIndex\n",
        "- Loaded documents from PDF\n",
        "- Used `VectorStoreIndex` for retrieval\n",
        "- Queried with `.query()` without custom prompt control\n",
        "- Responses were generic and one-tone\n",
        "\n",
        "### üî• Phase 2: Prompt-Augmented RAG\n",
        "- Introduced `ResponseSynthesizer` to control prompt formatting\n",
        "- Created custom `PromptTemplate`s for:\n",
        "  - `compare`\n",
        "  - `summarize`\n",
        "  - `elaborate`\n",
        "  - `creative`\n",
        "  - `chat` (default fallback)\n",
        "\n",
        "### üöÄ Phase 3: Dynamic Prompt Routing\n",
        "- Built a lightweight **tone/intent classifier**\n",
        "- Used LLM to classify user query type\n",
        "- Dynamically selected the appropriate prompt template\n",
        "- Created modular query flow: classify ‚Üí route ‚Üí synthesize ‚Üí answer\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Features\n",
        "\n",
        "- üìÑ PDF ingestion + embedding (Google GenAI or HuggingFace)\n",
        "- üîç Semantic search using Chroma vector store\n",
        "- üß© Custom response synthesis with prompt control\n",
        "- üîÑ Tone-aware query classification via LLM\n",
        "- üéØ Dynamic RAG with prompt routing\n",
        "- ‚ú® Clean, modular architecture for easy extension\n",
        "\n",
        "---\n",
        "\n",
        "## üñºÔ∏è Prompt Types Used\n",
        "\n",
        "| Prompt Type | Example Query |\n",
        "|-------------|---------------|\n",
        "| `compare`   | \"Compare top-k and top-p sampling\" |\n",
        "| `summarize` | \"List the key prompting techniques in the paper\" |\n",
        "| `elaborate` | \"Explain chain-of-thought reasoning in detail\" |\n",
        "| `creative`  | \"Describe prompt engineering using a cooking recipe analogy\" |\n",
        "| `chat`      | \"What is prompt engineering?\" |\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Tech Stack\n",
        "\n",
        "- üß† **LlamaIndex** ‚Äì RAG backbone\n",
        "- ‚úçÔ∏è **Google GenAI** ‚Äì LLM & embeddings (Gemini)\n",
        "- üß† **PromptTemplate** + `ResponseSynthesizer` ‚Äì Prompt control\n",
        "- üìö **Chroma** ‚Äì Vector store backend\n",
        "- üìÑ **PyMuPDF** ‚Äì PDF text parsing\n",
        "- üß™ **Python** ‚Äì Language of choice\n",
        "\n",
        "---\n",
        "\n",
        "#Built with love, curiosity, and a passion for building better AI Apps\n",
        "Thanks to [LlamaIndex](https://www.llamaindex.ai/) and open-source tools for making this powerful system possible.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KodcQxA3al9k"
      },
      "id": "KodcQxA3al9k"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface -U -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pPSXey7LuFg4"
      },
      "id": "pPSXey7LuFg4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-llms-google-genai llama-index"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UZc2NO58uLSg"
      },
      "id": "UZc2NO58uLSg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-google-genai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VUPrdLB6um22"
      },
      "id": "VUPrdLB6um22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here I used the Google Gemini API for the LLM and the embeddings. You can use any combination of LLM and embedding model from the llamaindex's documentation**"
      ],
      "metadata": {
        "id": "tgXvbCYUcHua"
      },
      "id": "tgXvbCYUcHua"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "GOOGLE_API_KEY = \"\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "evaH9X7wuM_S"
      },
      "id": "evaH9X7wuM_S",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "llm = GoogleGenAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "   api_key=\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "5Dq9L9EEuPCi"
      },
      "id": "5Dq9L9EEuPCi",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "74258ead295b4c2a95fe16c7ec19bf0b",
            "164e4b83d2c14a6ea8f447de3b25da64",
            "b5145ef54b5a485599293cc179641199",
            "8965fc4b3e9341ac9689205ab0fda395",
            "a26f6e35928940999b689de112201e42",
            "be8f2e1e9bde4cc087fd939f68a88d84",
            "9d286a2ee1e8483180fa962aed080779",
            "2bcd2ae8b763430a9bb5c54d3c58ebf5",
            "9ff673acf32b4af9b70ff955356cb8c8",
            "db1d1e792eec45e89a4e510478d46c84",
            "8c5ab72fff544f4998e717007118976e",
            "3aba9a85e4354ff0a7914ec1e7b379be",
            "986194c2bef546989df77334989b7dc0",
            "14d94a5d94824f158fe650eb01c1ff1b",
            "ef95026dacdf4d7d81ab2b7ad657c47a",
            "50db106488c74e1f8c245147b9628457",
            "d9bff9c6ffb2455792e36083560df8ac",
            "4a7135116516448a8c9d747a3f295f3a",
            "2abebb9272464e6f8859960e4b4e9b62",
            "3411800af1c54fe790536d039de10d47"
          ]
        },
        "id": "dRWZS3V3uTbF",
        "outputId": "497d3186-8621-4df1-ae49-3b432b06daed"
      },
      "id": "dRWZS3V3uTbF",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74258ead295b4c2a95fe16c7ec19bf0b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
        "from google.genai.types import EmbedContentConfig\n",
        "# Load document, put the correct directory name here . here I am doing it only for 1 pdf .\n",
        "reader = SimpleDirectoryReader(input_files=[\"Prompt_engineering.pdf\"])\n",
        "documents = reader.load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")\n",
        "\n",
        "# Split into chunks\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "# you can change the batch size , model_name to different varieties that google embeddings provide ,\n",
        "# go to https://ai.google.dev/gemini-api/docs/embeddings\n",
        "embed_model = GoogleGenAIEmbedding(\n",
        "    model_name=\"text-embedding-004\",\n",
        "    embed_batch_size=200,\n",
        "    api_key=\"\"\n",
        ")\n",
        "# Set up LLM and embedding model\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "# Create vector index\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "# Create query engine\n",
        "query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09tcyiHQudfG",
        "outputId": "ec82c30d-dd46-4f66-c1e5-ea16a5995b40"
      },
      "id": "09tcyiHQudfG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 68 document(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = vector_index.vector_store\n",
        "\n",
        "embedding_dict = vector_store.data.embedding_dict\n",
        "node_dict = vector_store.data.text_id_to_ref_doc_id\n",
        "print(f\"Number of embeddings: {len(embedding_dict)}\")\n",
        "print(f\"Number of node references: {len(node_dict)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5z650PmCdYS",
        "outputId": "577b8c49-09b6-458a-a398-658acd44de4a"
      },
      "id": "M5z650PmCdYS",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embeddings: 68\n",
            "Number of node references: 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we test the RAG System with prompts, this is done without prompt augmentation, by default it is handled by Llamaindex, but later we will see how we can add custom prompts. we wont have much control over default version.**"
      ],
      "metadata": {
        "id": "3R44PB0bdQJg"
      },
      "id": "3R44PB0bdQJg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb4a604f",
        "outputId": "ff9a8e2e-66d4-46a2-a394-a930ca867ec8"
      },
      "source": [
        "response = query_engine.query(\"What is Prompt Engineering ? \")\n",
        "print(response,end=' ')"
      ],
      "id": "eb4a604f",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt engineering is an iterative process of crafting effective prompts that considers word-choice, style and tone, structure, and context. It is an input that the model uses to predict a specific output. Everyone can write a prompt.\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = query_engine.query(\"What is difference between top-k and top-p ?\")\n",
        "print(response2,end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juTYwBqfDwbL",
        "outputId": "128cc48c-7ad8-4133-bb54-7b7f3922a2e9"
      },
      "id": "juTYwBqfDwbL",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-K sampling selects the top K most likely tokens from the model‚Äôs predicted distribution, while top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P).\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = query_engine.query(\"What are the various Prompting techniques in the paper?\")\n",
        "print(response3,end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFnfY7jRECxq",
        "outputId": "4b3b0be3-fb25-4f8b-9b16-0f8574aca69c"
      },
      "id": "TFnfY7jRECxq",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prompting techniques mentioned in the paper are: System, contextual, role, and step-back prompting.\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response4 = query_engine.query(\"What is the ReAct mentioned in the paper ? explain it briefly\")\n",
        "print(response4,end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXQNkd1XEbTZ",
        "outputId": "f1a95b40-9515-4e04-fc6b-66c76a4cf1e6"
      },
      "id": "wXQNkd1XEbTZ",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReAct is a prompting paradigm that enables LLMs to solve complex tasks by combining natural language reasoning with external tools, allowing the LLM to take actions like interacting with external APIs to retrieve information. It works by combining reasoning and acting into a thought-action loop. The LLM first reasons about the problem and generates a plan of action, then performs the actions in the plan and observes the results. The LLM then uses the observations to update its reasoning and generate a new plan of action, continuing until the LLM reaches a solution to the problem.\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response5 = query_engine.query(\"what is json repair ? explain it\")\n",
        "print(response5,end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJR0Iq4REm9g",
        "outputId": "ce0cf31f-ea0f-4271-e738-8cbc599b0393"
      },
      "id": "pJR0Iq4REm9g",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON repair is a tool, like the json-repair library on PyPI, that fixes incomplete or malformed JSON objects. It can automatically fix JSON outputs that are cut off due to token limits, which is helpful when working with LLM-generated JSON.\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The RAG System correctly identifies what the document is about and it will not answer those questions which are not related to it. This what is required and it works as obvious**"
      ],
      "metadata": {
        "id": "biIoEB-HdsGt"
      },
      "id": "biIoEB-HdsGt"
    },
    {
      "cell_type": "code",
      "source": [
        "response6 = query_engine.query(\"what is meditation ? how it benefits us ?\")\n",
        "print(response6,end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnDjlndzFAWg",
        "outputId": "9c365bc2-6d17-4c91-b575-3ecb7d246727"
      },
      "id": "OnDjlndzFAWg",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This document is about prompt engineering and does not contain information about meditation.\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response7 = query_engine.query(\"what is convolutional neural network(CNN) in deep learning ?\")\n",
        "print(response7,end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt3pjeTuFNC-",
        "outputId": "00021a6e-0c14-4bfa-98e3-dd19b1aa62aa"
      },
      "id": "Mt3pjeTuFNC-",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am sorry, but this document does not contain information about convolutional neural networks.\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response8 = query_engine.query(\"explain about the temperature parameter\")\n",
        "print(response8,end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttTaPSHpFgjL",
        "outputId": "412396aa-5bb8-461e-a83b-2b6a3b1a884d"
      },
      "id": "ttTaPSHpFgjL",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature regulates the amount of randomness employed when choosing tokens. Utilizing lower temperatures is suitable for prompts requiring a more predictable response, whereas higher temperatures can yield more varied or unexpected outcomes. A temperature of 0 corresponds to greedy decoding.\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From here onwards we will \"AUGMENT\" OUR CUSTOM PROMPTS AND HAVE MORE CONTROL OVER THE RAG SYSTEM. THIS FUNCTIONALITY IS PROVIDED BY VARIOUS LIBRARIES OF LLAMAINDEX. SO WE WILL SEE HOW TO DO IT**"
      ],
      "metadata": {
        "id": "lFPllTNod684"
      },
      "id": "lFPllTNod684"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.response_synthesizers import ResponseMode\n",
        "\n",
        "# we will define our own custom prompt template here\n",
        "custom_prompt = PromptTemplate(\n",
        "    \"You are a helpful research assistant. Based on the following context, answer the user's question.\\n\\n\"\n",
        "    \"----------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"----------------\\n\"\n",
        "    \"Question: {query_str}\\n\"\n",
        "    \"Answer:\"\n",
        ")\n",
        "\n",
        "# Build a response synthesizer using your prompt from the llamaindex docs\n",
        "# Reference : https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/\n",
        "synthesizer = get_response_synthesizer(\n",
        "    response_mode=ResponseMode.COMPACT,\n",
        "    text_qa_template=custom_prompt\n",
        ")\n",
        "\n",
        "# Get retriever from vector index\n",
        "retriever = vector_index.as_retriever(similarity_top_k=4)\n",
        "\n",
        "#   Plug into query engine\n",
        "custom_query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=synthesizer\n",
        ")\n",
        "\n",
        "#   Ask questions with prompt-injected RAG\n",
        "response = custom_query_engine.query(\"What is the difference between top-k and top-p sampling?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vv7ipmrI49R",
        "outputId": "424404f1-2398-4a47-f922-7606d16c9120"
      },
      "id": "-Vv7ipmrI49R",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-K sampling selects the top K most likely tokens from the model‚Äôs predicted distribution, while top-P sampling selects the top tokens whose cumulative probability does not exceed a certain value (P).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The above custom_prompt template might have not made much sense to you. so lets take a bullet_prompt template and test out with it . You can see the results. It is as expected in bullets. So you see the injection of any type of prompt works as expected. Now lets play with it to improve our RAG system**"
      ],
      "metadata": {
        "id": "XeywMXwCeRt_"
      },
      "id": "XeywMXwCeRt_"
    },
    {
      "cell_type": "code",
      "source": [
        "bullet_prompt = PromptTemplate(\n",
        "    \"Based on the context below, answer the question with a concise bullet-point summary.\\n\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"Question: {query_str}\\n\\n\"\n",
        "    \"Answer in bullets:\"\n",
        ")\n",
        "various_prompts(bullet_prompt,\"What is the difference between top-k and top-p sampling?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e1z2qDcJlYa",
        "outputId": "aca744f4-437a-42ed-eb00-836e489e1e38"
      },
      "id": "4e1z2qDcJlYa",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a summary of the differences between Top-K and Top-P sampling:\n",
            "\n",
            "*   **Top-K:** Selects the K most likely tokens from the model's predicted distribution. Higher K values lead to more creative/varied output, while lower K values result in more restrictive/factual output. A K of 1 is equivalent to greedy decoding.\n",
            "*   **Top-P:** Selects the top tokens whose cumulative probability does not exceed a certain value (P). P ranges from 0 (greedy decoding) to 1 (all tokens).\n",
            "*   **Combined Use:** Experimentation is recommended to determine which method (or both together) produces the desired results.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From here onwards we will start with the DYNAMIC AWARE ROUTING BASED RAG.\n",
        "First we define various type of prompts, and test for each individually. we can see it is as expected for each type of prompt. If it was naive RAG, then it would not have perfomed well as we saw above. Now after testing each of these we ask the LLM to identify the tone and then the prompt will be choosen and the answer will be given. So this improves its response and a good performance is obtained**"
      ],
      "metadata": {
        "id": "YJy8LQWQfO-a"
      },
      "id": "YJy8LQWQfO-a"
    },
    {
      "cell_type": "code",
      "source": [
        "elaborative_prompt = PromptTemplate(\n",
        "    \"You are a patient teacher. Use the following context to explain the user's question in simple and clear language.\\n\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"Question: {query_str}\\n\"\n",
        "    \"Answer as if teaching a beginner:\"\n",
        ")"
      ],
      "metadata": {
        "id": "EslCOgcnKDOw"
      },
      "id": "EslCOgcnKDOw",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_prompt = PromptTemplate(\n",
        "    \"Extract information based on the following context and user's query. Respond strictly in the following JSON format:\\n\"\n",
        "    \"{\\n\"\n",
        "    \"  \\\"question\\\": \\\"...\\\",\\n\"\n",
        "    \"  \\\"summary\\\": \\\"...\\\",\\n\"\n",
        "    \"  \\\"key_points\\\": [\\\"...\\\", \\\"...\\\", \\\"...\\\"]\\n\"\n",
        "    \"}\\n\\n\"\n",
        "    \"Context:\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"Question: {query_str}\"\n",
        ")"
      ],
      "metadata": {
        "id": "9WszzKHIKGqj"
      },
      "id": "9WszzKHIKGqj",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_prompt = PromptTemplate(\n",
        "    \"Using the following context, compare the two items mentioned in the question. Highlight differences clearly.\\n\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"Question: {query_str}\\n\\n\"\n",
        "    \"Answer in comparative form:\"\n",
        ")"
      ],
      "metadata": {
        "id": "ORHj0ApHKJEA"
      },
      "id": "ORHj0ApHKJEA",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creative_prompt = PromptTemplate(\n",
        "    \"Explain the concept in the user's question using a real-world analogy or metaphor. Base your answer on the context below.\\n\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"Question: {query_str}\\n\\n\"\n",
        "    \"Creative analogy:\"\n",
        ")"
      ],
      "metadata": {
        "id": "ryax5xQ7KLS1"
      },
      "id": "ryax5xQ7KLS1",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = PromptTemplate(\n",
        "    \"You are a helpful assistant. Use the context to respond to the user's query in a friendly, conversational tone.\\n\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"User: {query_str}\\n\"\n",
        "    \"Assistant:\"\n",
        ")"
      ],
      "metadata": {
        "id": "NyAtm9RyKO8d"
      },
      "id": "NyAtm9RyKO8d",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def various_prompts(template,user_query):\n",
        "    synthesizer = get_response_synthesizer(\n",
        "    response_mode=ResponseMode.COMPACT,\n",
        "    text_qa_template=template\n",
        "    )\n",
        "    #  Get retriever from vector index\n",
        "    retriever = vector_index.as_retriever(similarity_top_k=4)\n",
        "    # Plug into query engine\n",
        "    custom_query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=synthesizer\n",
        "    )\n",
        "    # Ask questions with prompt-injected RAG\n",
        "    response = custom_query_engine.query(user_query)\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "XYbVNP2hKXc7"
      },
      "id": "XYbVNP2hKXc7",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here you can see various examples of prompts in action. Later we dynamically route to the appropriate one because we dont need only one of them, we will need the right one according to the query of user and hence the Dynamic Routing and it leads to amazing results.**"
      ],
      "metadata": {
        "id": "AfogTSEliZkT"
      },
      "id": "AfogTSEliZkT"
    },
    {
      "cell_type": "code",
      "source": [
        "various_prompts(elaborative_prompt,\"Explain what temperature parameter means in language models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpfEe8NkK08C",
        "outputId": "6aab64d8-07aa-41ff-9a86-c94d2b170ec4"
      },
      "id": "JpfEe8NkK08C",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's break down what this document is saying about how Large Language Models (LLMs) generate text and how you can influence it.\n",
            "\n",
            "Imagine an LLM is like a super-smart parrot that can predict what words should come next in a sentence. But instead of just knowing one word, it knows how likely *many* different words are to follow. It assigns a probability to each word in its \"vocabulary.\"\n",
            "\n",
            "**Here's the core idea:**\n",
            "\n",
            "The document explains that LLMs don't just pick the *single most likely* word every time. Instead, they use a process called \"sampling\" to choose the next word based on these probabilities. You can control this sampling process using settings like:\n",
            "\n",
            "*   **Temperature:** This controls how \"random\" or \"creative\" the LLM is.\n",
            "\n",
            "    *   A **low temperature** (close to 0) makes the LLM pick the *most likely* words more often. This leads to more predictable and \"safe\" outputs. If two tokens have the same highest predicted probability, depending on how tiebreaking is implemented you may not always get the same output with temperature 0\n",
            "    *   A **high temperature** makes the LLM pick *less likely* words sometimes. This leads to more surprising, diverse, and potentially creative outputs. But be careful, too high and it becomes nonsensical! As temperature gets higher and higher, all tokens become equally likely to be the next predicted token.\n",
            "\n",
            "*   **Top-K:** Imagine the LLM makes a list of the probabilities of all possible next words. Top-K tells the LLM to only consider the *top K* most probable words on that list.\n",
            "\n",
            "    *   A **low Top-K** (like 1) means the LLM *always* picks the single most likely word.\n",
            "    *   A **high Top-K** means the LLM can pick from a larger pool of likely words, leading to more variation. If you set top-K extremely high, like to the size of the LLM‚Äôs vocabulary, any token with a nonzero probability of being the next token will meet the top-K criteria and none are selected out.\n",
            "\n",
            "*   **Top-P (Nucleus Sampling):** This is similar to Top-K, but instead of picking a fixed number of words, it picks the smallest set of words whose probabilities add up to at least a certain value (P).\n",
            "\n",
            "    *   A **low Top-P** (close to 0) is similar to a low Top-K; the LLM focuses on the most probable words. If you set top-P to 0 (or a very small value), most LLM sampling implementations will then only consider the most probable token to meet the top-P criteria, making temperature and top-K irrelevant.\n",
            "    *   A **high Top-P** (close to 1) lets the LLM consider a wider range of words. If you set top-P to 1, any token with a nonzero probability of being the next token will meet the top-P criteria, and none are selected out.\n",
            "\n",
            "**In short:** These settings let you control how much freedom the LLM has when generating text. Lower values make it more focused and predictable, while higher values make it more creative and surprising.\n",
            "\n",
            "**Important Note:** The document also warns about a \"repetition loop bug.\" This is when the LLM gets stuck repeating the same words or phrases. It can happen with both low and high temperatures, so you might need to experiment to find the right balance.\n",
            "\n",
            "**Example:**\n",
            "\n",
            "The document suggests these starting points:\n",
            "\n",
            "*   **Coherent but Creative:** Temperature = 0.2, Top-P = 0.95, Top-K = 30\n",
            "*   **Especially Creative:** Temperature = 0.9, Top-P = 0.99, Top-K = 40\n",
            "*   **Less Creative:** Temperature = 0.1, Top-P = 0.9, Top-K = 20\n",
            "*   **Single Correct Answer (e.g., Math):** Temperature = 0\n",
            "\n",
            "I hope this helps you understand how these settings work! Let me know if you have any other questions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "various_prompts(json_prompt,\"What are the limitations of prompt engineering?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM56LlNGK-_Q",
        "outputId": "a85734b3-e1ac-479a-c572-fa90632518e6"
      },
      "id": "pM56LlNGK-_Q",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"question\": \"What are the limitations of prompt engineering?\",\n",
            "  \"summary\": \"Inadequate prompts can lead to ambiguous, inaccurate responses, and can hinder the model‚Äôs ability to provide meaningful output. LLMs aren‚Äôt perfect; the clearer your prompt text, the better it is for the LLM to predict the next likely text.\",\n",
            "  \"key_points\": [\n",
            "    \"Inadequate prompts can lead to ambiguous, inaccurate responses.\",\n",
            "    \"Inadequate prompts can hinder the model‚Äôs ability to provide meaningful output.\",\n",
            "    \"LLMs aren‚Äôt perfect; the clearer your prompt text, the better it is for the LLM to predict the next likely text.\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "various_prompts(compare_prompt,\"What is the difference between ReAct and CoT ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3RQ_rCCLHiS",
        "outputId": "962a9f62-5cd3-4a7c-b4a7-3ffdb52d9b9b"
      },
      "id": "q3RQ_rCCLHiS",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Tree of Thoughts (ToT) vs. ReAct (Reason & Act)**\n",
            "\n",
            "Here's a comparison of Tree of Thoughts (ToT) and ReAct, highlighting their key differences based on the provided context:\n",
            "\n",
            "*   **Reasoning Approach:**\n",
            "\n",
            "    *   **ToT:** Explores multiple reasoning paths simultaneously by maintaining a tree of thoughts, where each thought is a coherent language sequence. It branches out from different nodes in the tree to explore different reasoning paths. It generalizes the concept of CoT prompting.\n",
            "    *   **ReAct:** Combines reasoning and acting in a thought-action loop. The LLM reasons about the problem, generates a plan of action, performs the actions, observes the results, and updates its reasoning based on the observations.\n",
            "\n",
            "*   **Action & External Tools:**\n",
            "\n",
            "    *   **ToT:** The context doesn't explicitly mention the use of external tools or actions in the same way as ReAct.\n",
            "    *   **ReAct:** Explicitly designed to use external tools (search, code interpreter, etc.) and perform actions (interacting with external APIs) to retrieve information. This is a key component of its approach.\n",
            "\n",
            "*   **Mimicking Human Behavior:**\n",
            "\n",
            "    *   **ToT:** Not explicitly mentioned in the context.\n",
            "    *   **ReAct:** Mimics how humans operate in the real world by reasoning verbally and taking actions to gain information.\n",
            "\n",
            "*   **Implementation:**\n",
            "\n",
            "    *   **ToT:** Requires a notebook to go into more detail.\n",
            "    *   **ReAct:** Requires code to be written, often using frameworks like Langchain and tools like VertexAI and Google Search.\n",
            "\n",
            "*   **Complexity:**\n",
            "\n",
            "    *   **ToT:** Well-suited for complex tasks that require exploration.\n",
            "    *   **ReAct:** Designed for complex tasks that benefit from reasoning and interaction with the environment.\n",
            "\n",
            "In essence, ToT focuses on exploring multiple reasoning paths internally, while ReAct focuses on a reasoning-action loop that involves interacting with external tools and the environment.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "various_prompts(json_prompt,\"What is cost of apple macbook ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq7b4SKPLOaI",
        "outputId": "72eabd27-f884-4835-9687-cd6a2ec0d550"
      },
      "id": "qq7b4SKPLOaI",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"question\": \"What is cost of apple macbook ?\",\n",
            "  \"summary\": \"This document is about prompt engineering techniques and does not contain information about the cost of an Apple Macbook.\",\n",
            "  \"key_points\": []\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "various_prompts(creative_prompt,\"Explain zero shot , few shot and one shot prompting \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6Hm7uk0LdCW",
        "outputId": "354d0a03-dead-4f80-b275-28afa89c0d2f"
      },
      "id": "R6Hm7uk0LdCW",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you're teaching a dog a new trick, like \"fetch.\"\n",
            "\n",
            "*   **Zero-shot prompting** is like telling the dog, \"Okay, fetch!\" without ever showing it what \"fetch\" means or giving any prior examples. You're hoping the dog somehow understands what you want based solely on the word itself.\n",
            "\n",
            "*   **One-shot prompting** is like showing the dog *once* what you want it to do. You throw the ball, the dog brings it back, and you say, \"Good, fetch!\" Now the dog has one example to learn from.\n",
            "\n",
            "*   **Few-shot prompting** is like showing the dog the \"fetch\" action multiple times. You throw the ball, the dog brings it back, you praise it, and repeat this several times. The dog sees the pattern and is more likely to understand and perform the trick correctly.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "various_prompts(chat_prompt,\"What‚Äôs the deal with chain-of-thought prompting?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7OySSOZLdGM",
        "outputId": "92082bed-883c-4fd2-a456-8bdcdb5c135a"
      },
      "id": "l7OySSOZLdGM",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain of Thought (CoT) prompting is a technique that improves the reasoning capabilities of Large Language Models (LLMs) by having them generate intermediate reasoning steps. This helps the LLM come up with more accurate answers, especially for complex tasks that need some reasoning before responding.\n",
            "\n",
            "Here's the deal with CoT:\n",
            "\n",
            "*   **How it works:** Instead of just asking a question, you prompt the LLM to \"think step by step\" or \"let's think step by step\" to guide it to break down the problem.\n",
            "*   **Benefits:**\n",
            "    *   It's effective and works well with readily available LLMs, so you don't need to fine-tune them.\n",
            "    *   It allows you to see the LLM's reasoning, which helps you understand how it arrived at the answer and identify any errors in its logic.\n",
            "    *   It can improve the consistency of your prompts across different LLM versions.\n",
            "*   **Downsides:** Because the LLM shows its reasoning, the responses can be longer, which means it could cost you more money and time.\n",
            "\n",
            "There are also different types of CoT prompting, such as zero-shot CoT (where you simply add \"Let's think step by step\" to the prompt) and few-shot CoT (where you provide one or more examples of the reasoning process).\n",
            "\n",
            "Does that explanation help clarify things for you?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we first detect the type of tone of the user's query and then we proceed to choose the prompt template**"
      ],
      "metadata": {
        "id": "M-Cre56riyYy"
      },
      "id": "M-Cre56riyYy"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "def detect_prompt_type(llm, user_query):\n",
        "    classification_prompt = PromptTemplate(\n",
        "        \"You are a prompt classifier.\\n\"\n",
        "        \"Given a user's question, classify the tone or intent. \"\n",
        "        # this line below was added later, you may remove and test it and then later add it\n",
        "        \"If the user uses phrases like ‚Äúexplain like I‚Äôm 5‚Äù, ‚Äúexplain to a child‚Äù, ‚Äúuse a metaphor‚Äù, or ‚Äúusing a story‚Äù ‚Äî classify as creative.\"\n",
        "        \"Choose one of the following:\\n\"\n",
        "        \"- compare\\n\"\n",
        "        \"- summarize\\n\"\n",
        "        \"- elaborate\\n\"\n",
        "        \"- creative\\n\"\n",
        "        \"- chat\\n\\n\"\n",
        "        \"User question: {query_str}\\n\"\n",
        "        \"Classification:\"\n",
        "    )\n",
        "\n",
        "    prompt = classification_prompt.format(query_str=user_query)\n",
        "    message = ChatMessage(role=MessageRole.USER, content=prompt)\n",
        "    response = llm.chat([message])\n",
        "    return response.message.content.strip().lower()\n"
      ],
      "metadata": {
        "id": "q9BhXwH4NU7M"
      },
      "id": "q9BhXwH4NU7M",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synthesizer_from_type(prompt_type):\n",
        "    if prompt_type == \"compare\":\n",
        "        template = compare_prompt\n",
        "    elif prompt_type == \"summarize\":\n",
        "        template = bullet_prompt\n",
        "    elif prompt_type == \"elaborate\":\n",
        "        template = elaborative_prompt\n",
        "    elif prompt_type == \"creative\":\n",
        "        template = creative_prompt\n",
        "    else:\n",
        "        template = chat_prompt  # default\n",
        "\n",
        "    return get_response_synthesizer(\n",
        "        response_mode=ResponseMode.COMPACT,\n",
        "        text_qa_template=template\n",
        "    )\n"
      ],
      "metadata": {
        "id": "AjD2miKINZh0"
      },
      "id": "AjD2miKINZh0",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_rag_query(user_query, vector_index, llm):\n",
        "    # Step 1: Detect tone/intent\n",
        "    prompt_type = detect_prompt_type(llm, user_query)\n",
        "    print(f\"‚§∑ Detected prompt type: {prompt_type}\")\n",
        "\n",
        "    # Step 2: Build synthesizer\n",
        "    synthesizer = get_synthesizer_from_type(prompt_type)\n",
        "\n",
        "    # Step 3: Create query engine\n",
        "    retriever = vector_index.as_retriever(similarity_top_k=4)\n",
        "    query_engine = RetrieverQueryEngine(\n",
        "        retriever=retriever,\n",
        "        response_synthesizer=synthesizer\n",
        "    )\n",
        "\n",
        "    # Step 4: Query\n",
        "    return query_engine.query(user_query)\n"
      ],
      "metadata": {
        "id": "eGlN6gnQNb0-"
      },
      "id": "eGlN6gnQNb0-",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here are the results and as you can see, its amazing how the results are. It detects the prompt type and then the appropriate response is generated. This is useful in many applications where we need to give the response for a particualr style of prompt. Including all the prompts in one big definition maybe costly as the context size will increase. So its better to give it like this. We address the limitation of naive RAG and take it to next level. The kind of responses are really great. This is a more intelligent aware RAG System that the user would like to interact with as compared to naive RAG**"
      ],
      "metadata": {
        "id": "62OSWbsJkif9"
      },
      "id": "62OSWbsJkif9"
    },
    {
      "cell_type": "code",
      "source": [
        "response_dynamic = dynamic_rag_query(\"Can you compare top-k and top-p sampling techniques?\", vector_index, llm)\n",
        "print(response_dynamic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcqIxpzlNhpz",
        "outputId": "b3765911-81e4-4853-8e93-2b004a2ba76c"
      },
      "id": "CcqIxpzlNhpz",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚§∑ Detected prompt type: compare\n",
            "Top-K and Top-P sampling are both methods used in LLMs to control the randomness and diversity of generated text, but they differ in how they select the next token.\n",
            "\n",
            "*   **Selection Criteria:** Top-K selects the top K most likely tokens from the model's predicted distribution, whereas Top-P selects the top tokens whose cumulative probability does not exceed a certain value (P).\n",
            "\n",
            "*   **Parameter Range:** Top-K uses an integer value (K) to define the number of tokens to consider, while Top-P uses a probability value (P) ranging from 0 to 1.\n",
            "\n",
            "*   **Output Behavior:** A higher Top-K leads to more creative and varied output, while a lower Top-K results in more restrictive and factual output. Top-P values closer to 1 include more tokens, increasing randomness, while values closer to 0 restrict the selection to the most probable tokens.\n",
            "\n",
            "*   **Greedy Decoding Equivalence:** A Top-K of 1 is equivalent to greedy decoding, while a Top-P of 0 (or a very small value) often leads to only the most probable token being considered, also resembling greedy decoding.\n",
            "\n",
            "*   **Interaction with Temperature:** When temperature, Top-K, and Top-P are all available, tokens meeting both Top-K and Top-P criteria are considered, and then temperature is applied. If only one of Top-K or Top-P is available, only that setting is used.\n",
            "\n",
            "*   **Irrelevance under Extreme Temperature:** If temperature is set to 0, both Top-K and Top-P become irrelevant, as the most probable token is always selected. However, if temperature is extremely high, Top-K and Top-P determine the candidate tokens, and then a random selection occurs from those candidates.\n",
            "\n",
            "*   **Impact of Extreme Settings:** Setting Top-K to 1 makes temperature and Top-P irrelevant, as only one token is considered. Setting Top-K to the size of the LLM's vocabulary makes it irrelevant. Similarly, setting Top-P to 1 makes it irrelevant.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_dynamic_2 = dynamic_rag_query(\"what is prompt engineering in simple words ?\", vector_index, llm)\n",
        "print(response_dynamic_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G2oBWRSNzEK",
        "outputId": "7942e19c-b9c9-4238-96c2-78dfe75c230b"
      },
      "id": "9G2oBWRSNzEK",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚§∑ Detected prompt type: summarize\n",
            "*   Prompt engineering is crafting effective text inputs (prompts) for large language models (LLMs) to get accurate outputs.\n",
            "*   It's an iterative process that involves optimizing word choice, style, structure, and context.\n",
            "*   The goal is to guide the LLM to predict the right sequence of tokens and produce relevant results.\n",
            "*   Automatic Prompt Engineering (APE) automates the prompt creation process by using a model to generate and evaluate prompts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we are seeing the detected type as \"elaborate\", it is because in the elaborate prompt we wrote \"Answer as if teaching a beginner\" , so maybe it detected it as elaborate. well this response is also good . but later we will see how to make it creative**"
      ],
      "metadata": {
        "id": "bPBVxCD0lhmc"
      },
      "id": "bPBVxCD0lhmc"
    },
    {
      "cell_type": "code",
      "source": [
        "response_dynamic_2 = dynamic_rag_query(\"explain chain of thought prompting as if you are explaining it to a child\", vector_index, llm)\n",
        "print(response_dynamic_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l39c1yROfkz",
        "outputId": "c4efe61e-c46c-408d-e10f-e9500e5bd358"
      },
      "id": "0l39c1yROfkz",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚§∑ Detected prompt type: elaborate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llms.google_genai.utils:Retrying llama_index.llms.google_genai.base.GoogleGenAI._chat in 0.7774996118145691 seconds as it raised ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I'll explain Chain of Thought (CoT) prompting like I'm talking to a child.\n",
            "\n",
            "Imagine you're trying to solve a puzzle, like figuring out how many cookies your friend has.\n",
            "\n",
            "**Without Chain of Thought:** Someone just asks the computer, \"How many cookies does my friend have?\" The computer might guess and get it wrong!\n",
            "\n",
            "**With Chain of Thought:**  Instead of just guessing, we teach the computer to think step-by-step, like this:\n",
            "\n",
            "1.  **First, we tell the computer:** \"My friend started with 5 cookies.\"\n",
            "2.  **Then, we say:** \"Then, they got 3 more cookies.\"\n",
            "3.  **Finally, we ask:** \"So, how many cookies do they have now?\"\n",
            "\n",
            "Now, the computer can think: \"Okay, 5 cookies + 3 cookies = 8 cookies!\"  It figures it out step-by-step and gets the right answer!\n",
            "\n",
            "**So, Chain of Thought is like teaching the computer to think through a problem, one step at a time, instead of just guessing. This helps it get the right answer, especially when the problem is a little tricky!**\n",
            "\n",
            "The document you provided also mentions these key things about Chain of Thought:\n",
            "\n",
            "*   **It's helpful:** It makes the computer smarter at solving problems.\n",
            "*   **It shows its work:** You can see the steps the computer took to get the answer, so you can understand how it figured it out.\n",
            "*   **It can cost more:** Because the computer is writing out all the steps, it takes a little longer and might cost a bit more (like using more paper to write out your work).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_dynamic_4 = dynamic_rag_query(\"Describe prompt engineering using a cooking recipe analogy.\", vector_index, llm)\n",
        "print(response_dynamic_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km6OfVAkO-68",
        "outputId": "8dda4453-4cd2-4227-a03d-60d59868b6b6"
      },
      "id": "Km6OfVAkO-68",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚§∑ Detected prompt type: creative\n",
            "Based on the provided context, prompt engineering is like crafting a detailed recipe for a chef (the language model).\n",
            "\n",
            "*   **The Prompt is the Recipe:** Just like a recipe provides instructions, ingredients, and cooking methods, a prompt provides the language model with the necessary information to generate a specific output.\n",
            "*   **Inadequate Prompts are like Vague Recipes:** If a recipe is poorly written or missing key information, the resulting dish might be a disaster. Similarly, inadequate prompts can lead to ambiguous, inaccurate, or unhelpful responses from the language model.\n",
            "*   **Iterative Process is like Refining a Recipe:** A chef might need to experiment and adjust a recipe multiple times to perfect it. Similarly, prompt engineering is an iterative process where you refine your prompts based on the model's output to achieve the desired results.\n",
            "*   **Examples in Prompts are like Pictures in a Recipe Book:** Including examples in your prompt (like the pizza example) is like showing pictures of the finished dish in a recipe book. It helps the model understand the desired output format and style.\n",
            "*   **Automatic Prompt Engineering is like a Recipe Generator:** Instead of manually writing recipes, you use a program to generate multiple recipe variations. You then evaluate and refine the best ones. This automates the process of finding the perfect recipe.\n",
            "*   **Edge Cases are like Unusual Ingredients:** Including edge cases in your examples is like adding unusual or unexpected ingredients to a recipe to see how the chef handles them. This helps ensure the model can handle a variety of inputs.\n",
            "*   **Temperature and Top-k/Top-p are like Oven Settings:** Adjusting the temperature and top-k/top-p values is like adjusting the oven settings. Too low, and the dish might be undercooked (deterministic output). Too high, and it might burn (random output). Finding the right balance is crucial for a perfect result.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now here we see the type is creative, so it was all in the prompt. So it is never that in the first trial you will get the correct responses. you may have to change the prompt. Give it more explanation. That's where the magic of prompt lies. It is really interesting to see how this happens. So experiment, change the prompt and see yourself, how the results improve and you get better results**"
      ],
      "metadata": {
        "id": "nDhtCU48mFIr"
      },
      "id": "nDhtCU48mFIr"
    },
    {
      "cell_type": "code",
      "source": [
        "response_dynamic_5 = dynamic_rag_query(\"explain to a child what is chain of thought prompting ?\", vector_index, llm)\n",
        "print(response_dynamic_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSzLtSS3QGy4",
        "outputId": "80c1aa76-058e-488a-fed7-d729ac11a7ec"
      },
      "id": "wSzLtSS3QGy4",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚§∑ Detected prompt type: creative\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.llms.google_genai.utils:Retrying llama_index.llms.google_genai.base.GoogleGenAI._chat in 0.35676464399418584 seconds as it raised ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you're trying to build a really cool Lego castle.\n",
            "\n",
            "**Without Chain of Thought:** You just dump all the Lego bricks on the table and try to guess where each one goes. You might get lucky and stick a few together, but it's mostly random and probably won't look like a castle. This is like asking the computer a complicated question without giving it any steps to follow. It might give you an answer, but it's probably wrong or doesn't make sense.\n",
            "\n",
            "**With Chain of Thought:** Now, imagine you have a set of instructions. First, you build the base. Then, you add the walls. Next, you put on the towers, and finally, you add the flags. Each step helps you get closer to the finished castle. Chain of Thought is like giving the computer those instructions. You tell it to \"think step-by-step\" and break down the problem into smaller, easier-to-solve pieces. This way, it's much more likely to build the correct \"castle\" (give the right answer).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The thing we changed in the custom prompt template was just this we added the line ```If the user uses phrases like ‚Äúexplain like I‚Äôm 5‚Äù, ‚Äúexplain to a child‚Äù, ‚Äúuse a metaphor‚Äù, or ‚Äúusing a story‚Äù ‚Äî classify as creative.```. So this is the magic of prompt. Try changing and see what works best for your use case and build a intelligent system !!!**\n",
        "\n",
        "```\n",
        " classification_prompt = PromptTemplate(\n",
        "        \"You are a prompt classifier.\\n\"\n",
        "        \"Given a user's question, classify the tone or intent. \"\n",
        "        \"If the user uses phrases like ‚Äúexplain like I‚Äôm 5‚Äù, ‚Äúexplain to a child‚Äù, ‚Äúuse a metaphor‚Äù, or ‚Äúusing a story‚Äù ‚Äî classify as creative.\"\n",
        "        \"Choose one of the following:\\n\"\n",
        "        \"- compare\\n\"\n",
        "        \"- summarize\\n\"\n",
        "        \"- elaborate\\n\"\n",
        "        \"- creative\\n\"\n",
        "        \"- chat\\n\\n\"\n",
        "        \"User question: {query_str}\\n\"\n",
        "        \"Classification:\"\n",
        "    )\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "T6Vw1xsNmqJx"
      },
      "id": "T6Vw1xsNmqJx"
    },
    {
      "cell_type": "code",
      "source": [
        "response_dynamic_6 = dynamic_rag_query(\"what is the difference between Tree of Thoughts and  ReAct(reason and act) ?. give the difference in bullet points\", vector_index, llm)\n",
        "print(response_dynamic_6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVNplcRTWLuj",
        "outputId": "20a953d3-4d31-4652-fc87-4a37ee1bab44"
      },
      "id": "mVNplcRTWLuj",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚§∑ Detected prompt type: compare\n",
            "Here's a comparison of Tree of Thoughts (ToT) and ReAct (Reason & Act) based on the provided context, highlighting their key differences:\n",
            "\n",
            "**Tree of Thoughts (ToT)**\n",
            "\n",
            "*   **Reasoning Approach:** Explores multiple reasoning paths simultaneously by maintaining a tree of thoughts. Each thought represents a coherent language sequence as an intermediate step.\n",
            "*   **Generalization of CoT:** Generalizes the concept of Chain of Thought (CoT) prompting.\n",
            "*   **Exploration Focus:** Well-suited for complex tasks that require exploration of different reasoning paths.\n",
            "*   **Output:** Generates multiple Chains of Thoughts.\n",
            "*   **Consistency:** Aims to improve accuracy by considering multiple perspectives and selecting the most consistent answer.\n",
            "\n",
            "**ReAct (Reason & Act)**\n",
            "\n",
            "*   **Reasoning Approach:** Combines natural language reasoning with external tools (search, code interpreter, etc.) in a thought-action loop.\n",
            "*   **Action-Oriented:** Enables LLMs to perform actions, such as interacting with external APIs to retrieve information.\n",
            "*   **Mimics Human Behavior:** Mimics how humans reason and take actions to gain information.\n",
            "*   **Thought-Action Loop:** Works by combining reasoning and acting into a thought-action loop, where the LLM reasons, plans, acts, observes, and updates its reasoning.\n",
            "*   **External Interaction:** Allows the LLM to interact with the external world through tools and APIs.\n",
            "\n",
            "**Key Differences Summarized in Bullet Points:**\n",
            "\n",
            "*   **Exploration vs. Action:** ToT focuses on exploring multiple reasoning paths, while ReAct focuses on reasoning and taking actions in the real world.\n",
            "*   **Internal vs. External:** ToT primarily operates internally within the LLM's reasoning process, while ReAct involves external interactions with tools and APIs.\n",
            "*   **Tree Structure vs. Loop:** ToT uses a tree structure to represent different reasoning paths, while ReAct uses a thought-action loop to iteratively reason and act.\n",
            "*   **Generalization vs. Paradigm:** ToT generalizes the concept of CoT, while ReAct is a paradigm for enabling LLMs to solve complex tasks.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**See the scores of Top-4 nodes. You can see here which chunk(s) were picked to give the answer**"
      ],
      "metadata": {
        "id": "EMa0x2GxnNkh"
      },
      "id": "EMa0x2GxnNkh"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Source nodes:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, node in enumerate(response_dynamic_6.source_nodes):\n",
        "    print(f\"Node {i+1}:\")\n",
        "    print(f\"Score: {node.score}\")\n",
        "    print(f\"Text: {node.text}\")\n",
        "    print(f\"Metadata: {node.metadata}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS3wxb2PdAwR",
        "outputId": "fdeedac7-04ae-4cc8-87bb-7024653ca3a8"
      },
      "id": "cS3wxb2PdAwR",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source nodes:\n",
            "======================================================================\n",
            "Node 1:\n",
            "Score: 0.7206183639768341\n",
            "Text: Prompt Engineering\n",
            "February 2025\n",
            "37\n",
            "This approach makes ToT particularly well-suited for complex tasks that require exploration. It \n",
            "works by maintaining a tree of thoughts, where each thought represents a coherent language \n",
            "sequence that serves as an intermediate step toward solving a problem. The model can then \n",
            "explore different reasoning paths by branching out from different nodes in the tree. \n",
            "There‚Äôs a great notebook, which goes into a bit more detail showing The Tree of Thought \n",
            "(ToT) which is based on the paper ‚ÄòLarge Language Model Guided Tree-of-Thought‚Äô.9\n",
            "ReAct (reason & act)\n",
            "Reason and act (ReAct) [10]13 prompting is a paradigm for enabling LLMs to solve complex \n",
            "tasks using natural language reasoning combined with external tools (search, code \n",
            "interpreter etc.) allowing the LLM to perform certain actions, such as interacting with external \n",
            "APIs to retrieve information which is a first step towards agent modeling.\n",
            "ReAct mimics how humans operate in the real world, as we reason verbally and can \n",
            "take actions to gain information. ReAct performs well against other prompt engineering \n",
            "approaches in a variety of domains.\n",
            "ReAct prompting works by combining reasoning and acting into a thought-action loop. The \n",
            "LLM first reasons about the problem and generates a plan of action. It then performs the \n",
            "actions in the plan and observes the results. The LLM then uses the observations to update \n",
            "its reasoning and generate a new plan of action. This process continues until the LLM \n",
            "reaches a solution to the problem.\n",
            "To see this in action, you need to write some code. In code Snippet 1 I am using the langchain \n",
            "framework for Python, together with VertexAI (google-cloud-aiplatform) and the \n",
            "google-search-results pip packages.\n",
            "Metadata: {'page_label': '37', 'file_name': 'Prompt_engineering.pdf', 'file_path': 'Prompt_engineering.pdf', 'file_type': 'application/pdf', 'file_size': 6816989, 'creation_date': '2025-07-03', 'last_modified_date': '2025-07-03'}\n",
            "------------------------------------------------------------\n",
            "Node 2:\n",
            "Score: 0.6249667271750164\n",
            "Text: Prompt Engineering\n",
            "February 2025\n",
            "36\n",
            "By generating many Chains of Thoughts, and taking the most commonly occurring answer \n",
            "(‚ÄúIMPORTANT‚Äù), we can get a more consistently correct answer from the LLM.\n",
            "This example shows how self-consistency prompting can be used to improve the accuracy \n",
            "of an LLM‚Äôs response by considering multiple perspectives and selecting the most \n",
            "consistent answer.\n",
            "Tree of Thoughts (ToT)\n",
            "Now that we are familiar with chain of thought and self-consistency prompting, let‚Äôs review \n",
            "Tree of Thoughts (ToT).12 It generalizes the concept of CoT prompting because it allows LLMs \n",
            "to explore multiple different reasoning paths simultaneously, rather than just following a \n",
            "single linear chain of thought. This is depicted in Figure 1.\n",
            "Figure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on \n",
            "the right\n",
            "Metadata: {'page_label': '36', 'file_name': 'Prompt_engineering.pdf', 'file_path': 'Prompt_engineering.pdf', 'file_type': 'application/pdf', 'file_size': 6816989, 'creation_date': '2025-07-03', 'last_modified_date': '2025-07-03'}\n",
            "------------------------------------------------------------\n",
            "Node 3:\n",
            "Score: 0.6223723936701856\n",
            "Text: Step-back prompting 25\n",
            "Chain of Thought (CoT) 29\n",
            "Self-consistency 32\n",
            "Tree of Thoughts (ToT) 36\n",
            "ReAct (reason & act) 37\n",
            "Automatic Prompt Engineering 40\n",
            "Code prompting 42\n",
            "Prompts for writing code 42\n",
            "Prompts for explaining code 44\n",
            "Prompts for translating code 46\n",
            "Prompts for debugging and reviewing code 48\n",
            "What about multimodal prompting? 54\n",
            "Best Practices 54\n",
            "Provide examples 54\n",
            "Design with simplicity 55\n",
            "Be specific about the output 56\n",
            "Use Instructions over Constraints 56\n",
            "Control the max token length 58\n",
            "Use variables in prompts 58\n",
            "Experiment with input formats and writing styles 59\n",
            "For few-shot prompting with classification tasks, mix up the classes 59\n",
            "Adapt to model updates 60\n",
            "Experiment with output formats 60\n",
            "Metadata: {'page_label': '4', 'file_name': 'Prompt_engineering.pdf', 'file_path': 'Prompt_engineering.pdf', 'file_type': 'application/pdf', 'file_size': 6816989, 'creation_date': '2025-07-03', 'last_modified_date': '2025-07-03'}\n",
            "------------------------------------------------------------\n",
            "Node 4:\n",
            "Score: 0.5379150097243947\n",
            "Text: Prompt Engineering\n",
            "February 2025\n",
            "29\n",
            "Chain of Thought (CoT)\n",
            "Chain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities \n",
            "of LLMs by generating intermediate reasoning steps. This helps the LLM generate more \n",
            "accurate answers. You can combine it with few-shot prompting to get better results on more \n",
            "complex tasks that require reasoning before responding as it‚Äôs a challenge with a zero-shot \n",
            "chain of thought.\n",
            "CoT has a lot of advantages. First of all, it‚Äôs low-effort while being very effective and works \n",
            "well with off-the-shelf LLMs (so no need to finetune). You also get interpretability with CoT \n",
            "prompting, as you can learn from the LLM‚Äôs responses and see the reasoning steps that were \n",
            "followed. If there‚Äôs a malfunction, you will be able to identify it. Chain of thought appears \n",
            "to improve robustness when moving between different LLM versions. Which means the \n",
            "performance of your prompt should drift less between different LLMs than if your prompt \n",
            "does not use reasoning chains. Of course there are also disadvantages, but they are \n",
            "somewhat intuitive.\n",
            "The LLM response includes the chain of thought reasoning, which means more output \n",
            "tokens, which means predictions cost more money and take longer.\n",
            "To explain the following example in Table 11, let‚Äôs first try to create a prompt that is not using \n",
            "CoT prompting to showcase the flaws of a large language model.\n",
            "Prompt When I was 3 years old, my partner was 3 times my age. Now, I \n",
            "am 20 years old. How old is my partner?\n",
            "Output 63 years old\n",
            "Table 11. An example of a prompt which is trying to solve a mathematical problem\n",
            "Metadata: {'page_label': '29', 'file_name': 'Prompt_engineering.pdf', 'file_path': 'Prompt_engineering.pdf', 'file_type': 'application/pdf', 'file_size': 6816989, 'creation_date': '2025-07-03', 'last_modified_date': '2025-07-03'}\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc-autonumbering": false,
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "74258ead295b4c2a95fe16c7ec19bf0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_9d286a2ee1e8483180fa962aed080779"
          }
        },
        "164e4b83d2c14a6ea8f447de3b25da64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bcd2ae8b763430a9bb5c54d3c58ebf5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9ff673acf32b4af9b70ff955356cb8c8",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b5145ef54b5a485599293cc179641199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_db1d1e792eec45e89a4e510478d46c84",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8c5ab72fff544f4998e717007118976e",
            "value": ""
          }
        },
        "8965fc4b3e9341ac9689205ab0fda395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_3aba9a85e4354ff0a7914ec1e7b379be",
            "style": "IPY_MODEL_986194c2bef546989df77334989b7dc0",
            "value": true
          }
        },
        "a26f6e35928940999b689de112201e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_14d94a5d94824f158fe650eb01c1ff1b",
            "style": "IPY_MODEL_ef95026dacdf4d7d81ab2b7ad657c47a",
            "tooltip": ""
          }
        },
        "be8f2e1e9bde4cc087fd939f68a88d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50db106488c74e1f8c245147b9628457",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d9bff9c6ffb2455792e36083560df8ac",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "9d286a2ee1e8483180fa962aed080779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "2bcd2ae8b763430a9bb5c54d3c58ebf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff673acf32b4af9b70ff955356cb8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db1d1e792eec45e89a4e510478d46c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c5ab72fff544f4998e717007118976e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3aba9a85e4354ff0a7914ec1e7b379be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986194c2bef546989df77334989b7dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14d94a5d94824f158fe650eb01c1ff1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef95026dacdf4d7d81ab2b7ad657c47a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "50db106488c74e1f8c245147b9628457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9bff9c6ffb2455792e36083560df8ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a7135116516448a8c9d747a3f295f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2abebb9272464e6f8859960e4b4e9b62",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3411800af1c54fe790536d039de10d47",
            "value": "Connecting..."
          }
        },
        "2abebb9272464e6f8859960e4b4e9b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3411800af1c54fe790536d039de10d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}