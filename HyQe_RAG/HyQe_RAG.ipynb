{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 HyQe RAG: Hypothetical Query Embedding Retrieval-Augmented Generation\n",
    "\n",
    "This project implements the **HyQe RAG** (Hypothetical Query Embedding Retrieval-Augmented Generation) technique, inspired by and following the methodology described in the research paper:  \n",
    "[HyQe: Hypothetical Query Embeddings for Enhanced Retrieval-Augmented Generation](https://arxiv.org/pdf/2410.15262) 📄.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Overview\n",
    "\n",
    "**HyQe RAG** enhances traditional RAG pipelines by generating hypothetical queries for each document chunk, embedding them, and using these embeddings to improve retrieval relevance. This approach enables more robust and context-aware retrieval, especially for complex or ambiguous user queries.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Implementation Details\n",
    "\n",
    "- **Document Loading & Splitting:**  \n",
    "    Documents are loaded from the `data` directory and split into manageable chunks using a sentence splitter.\n",
    "\n",
    "- **Embedding Models:**  \n",
    "    - Uses Google GenAI for both LLM (for hypothetical query generation) and embedding models.\n",
    "    - Embeddings are generated for both user queries and hypothetical queries.\n",
    "\n",
    "- **Hypothetical Query Generation:**  \n",
    "    For each document chunk, the LLM generates diverse, short hypothetical questions that could be answered by the chunk. These are then embedded and paired with their source chunk.\n",
    "\n",
    "- **Retrieval & Re-ranking:**  \n",
    "    - At query time, the user query is embedded.\n",
    "    - Each document chunk is scored based on its original retrieval score and its maximum similarity to any of its hypothetical queries.\n",
    "    - Chunks are re-ranked using a weighted combination of these scores.\n",
    "\n",
    "- **Answer Generation:**  \n",
    "    The top-ranked chunks are used as context for the LLM to generate a final answer.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Key Features\n",
    "\n",
    "- **Enhanced Retrieval:**  \n",
    "    By leveraging hypothetical queries, the system retrieves more relevant and contextually appropriate chunks.\n",
    "\n",
    "- **Plug-and-Play:**  \n",
    "    Easily adaptable to different embedding models and LLMs.\n",
    "\n",
    "- **Caching:**  \n",
    "    Embedding pairs are cached for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Usage\n",
    "\n",
    "1. **Configure API Keys:**  \n",
    "     Set your Google API key in the environment.\n",
    "\n",
    "2. **Load and Index Documents:**  \n",
    "     Documents are loaded, split, and indexed with hypothetical query embeddings.\n",
    "\n",
    "3. **Query:**  \n",
    "     Enter a natural language question. The system retrieves, re-ranks, and generates an answer using the HyQe RAG pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Reference\n",
    "\n",
    "This implementation is directly inspired by and follows the methodology from:  \n",
    "**HyQe: Hypothetical Query Embeddings for Enhanced Retrieval-Augmented Generation**  \n",
    "[https://arxiv.org/pdf/2410.15262](https://arxiv.org/pdf/2410.15262)\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Acknowledgements\n",
    "\n",
    "- Thanks to the authors of the HyQe paper for their innovative approach.\n",
    "- Built using [LangChain](https://python.langchain.com/), [LlamaIndex](https://www.llamaindex.ai/), and Google GenAI APIs.\n",
    "\n",
    "---\n",
    "\n",
    "> 💡 **Note:** This notebook is a research-inspired implementation and may require further tuning for production use.  \n",
    "> For more details, see the referenced paper above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GOOGLE_API_KEY = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "splitter = SentenceSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "embed_model = GoogleGenAIEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    embed_batch_size=100,\n",
    "    api_key=\"\"\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=retriever.retrieve(\"what is a COCOMO model ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "COCOMO Model (CONT.)\n",
      "Software cost estimation is done \n",
      "through three stages: \n",
      "Basic COCOMO,\n",
      "Intermediate COCOMO,  \n",
      "Complete COCOMO.\n",
      "0.7118802550909354\n",
      "\n",
      "31\n",
      "COCOMO Model\n",
      "COCOMO (COnstructive COst MOdel) \n",
      "proposed by Boehm.\n",
      "Divides software product \n",
      "developments into 3 categories: \n",
      "Organic \n",
      "Semidetached \n",
      "Embedded\n",
      "0.7005425318177478\n",
      "\n",
      "40\n",
      "Basic COCOMO Model (CONT .)\n",
      "Effort is \n",
      "somewhat \n",
      "super-linear in \n",
      "problem  size. \n",
      "Effort\n",
      "Size\n",
      "0.6912063002884875\n",
      "\n",
      "42\n",
      "Basic COCOMO Model (CONT.)\n",
      "Development time does not \n",
      "increase linearly with product size:\n",
      "For larger products more parallel \n",
      "activities can be identified:\n",
      "can be carried out simultaneously by a \n",
      "number of engineers.\n",
      "0.6894930106522041\n",
      "\n",
      "50\n",
      "Shortcoming of  basic and \n",
      "intermediate COCOMO models\n",
      "Both models:\n",
      "consider a software product as a single \n",
      "homogeneous entity:\n",
      "However, most large systems are made up of \n",
      "several smaller sub-systems.\n",
      "Some sub-systems may be considered as organic \n",
      "type, some may be considered embedded, etc.\n",
      "for some the reliability requirements may be high, \n",
      "and so on.\n",
      "0.6885727360849511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "  print(node.text)\n",
    "  print(node.score)\n",
    "  print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pairs = []\n",
    "def generate_hypothetical_query_embeddings(nodes, llm, embed_model):\n",
    "    global embedding_pairs\n",
    "\n",
    "    for node in nodes:\n",
    "        prompt = f\"\"\"\n",
    "        Which kinds of questions can be answered based on the following passage?\n",
    "\n",
    "        Passage:\n",
    "        \\\"\\\"\\\"{node.text}\\\"\\\"\\\"\n",
    "\n",
    "        Questions must be short, diverse, and each written on a separate line.\n",
    "        If the passage provides no meaningful content, respond with 'No Content'.\n",
    "        \"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        response_text = response if isinstance(response, str) else response.content\n",
    "\n",
    "        if \"No Content\" in response_text:\n",
    "            continue\n",
    "\n",
    "        questions = [q.strip() for q in response_text.split('\\n') if q.strip()]\n",
    "        if not questions:\n",
    "            continue\n",
    "\n",
    "        question_embeddings = embed_model.get_text_embedding_batch(questions)\n",
    "        for embedding in question_embeddings:\n",
    "            embedding_pairs.append((embedding, node))  # or store question too if needed\n",
    "\n",
    "    return embedding_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pairs= generate_hypothetical_query_embeddings(nodes,llm,embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "def list_to_embedding_map(embedding_pairs):\n",
    "    emb_map = defaultdict(list)\n",
    "    for emb, node in embedding_pairs:\n",
    "        emb_map[node.node_id].append(np.array(emb))\n",
    "    return emb_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open(\"hypo_cache.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embedding_pairs, f)\n",
    "\n",
    "# Load\n",
    "with open(\"hypo_cache.pkl\", \"rb\") as f:\n",
    "    embedding_pairs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_final_scores(nodes, embedding_pairs, query_embedding, lamda=0.75):\n",
    "    # Normalize query embedding for the cosine similarity\n",
    "    query_vec = np.array(query_embedding).reshape(1, -1)\n",
    "    query_vec /= np.linalg.norm(query_vec)\n",
    "    from collections import defaultdict\n",
    "    hypo_map = defaultdict(list)\n",
    "    for emb, src_node in embedding_pairs:\n",
    "        if src_node.node_id:  # in case node has ID\n",
    "            hypo_map[src_node.node_id].append(np.array(emb))\n",
    "    # For each node, find its maximum similarityy to any hypothetical query\n",
    "    for node in nodes:\n",
    "        node_id = node.node_id\n",
    "        max_sim = 0\n",
    "        if node_id in hypo_map:\n",
    "            hq_embs = np.array(hypo_map[node_id])\n",
    "            hq_embs = hq_embs / np.linalg.norm(hq_embs, axis=1, keepdims=True)  # normalize\n",
    "            sims = cosine_similarity(query_vec, hq_embs)[0]\n",
    "            max_sim = np.max(sims)\n",
    "        # Final score\n",
    "        node.score = node.score + lamda * max_sim\n",
    "\n",
    "    # Sort and return\n",
    "    nodes.sort(key=lambda x: x.score, reverse=True)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"what is a COCOMO model ?\"\n",
    "query_embedding=embed_model.get_text_embedding(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_final=compute_final_scores(nodes,embedding_pairs,query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "COCOMO Model\n",
      "COCOMO (COnstructive COst MOdel) \n",
      "proposed by Boehm.\n",
      "Divides software product \n",
      "developments into 3 categories: \n",
      "Organic \n",
      "Semidetached \n",
      "Embedded\n",
      "1.4192215507775896\n",
      "\n",
      "50\n",
      "Shortcoming of  basic and \n",
      "intermediate COCOMO models\n",
      "Both models:\n",
      "consider a software product as a single \n",
      "homogeneous entity:\n",
      "However, most large systems are made up of \n",
      "several smaller sub-systems.\n",
      "Some sub-systems may be considered as organic \n",
      "type, some may be considered embedded, etc.\n",
      "for some the reliability requirements may be high, \n",
      "and so on.\n",
      "1.3458826537200281\n",
      "\n",
      "35\n",
      "COCOMO Model (CONT.)\n",
      "Software cost estimation is done \n",
      "through three stages: \n",
      "Basic COCOMO,\n",
      "Intermediate COCOMO,  \n",
      "Complete COCOMO.\n",
      "1.3343138428704036\n",
      "\n",
      "40\n",
      "Basic COCOMO Model (CONT .)\n",
      "Effort is \n",
      "somewhat \n",
      "super-linear in \n",
      "problem  size. \n",
      "Effort\n",
      "Size\n",
      "1.314182786440845\n",
      "\n",
      "42\n",
      "Basic COCOMO Model (CONT.)\n",
      "Development time does not \n",
      "increase linearly with product size:\n",
      "For larger products more parallel \n",
      "activities can be identified:\n",
      "can be carried out simultaneously by a \n",
      "number of engineers.\n",
      "1.3124694968045616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _node in nodes_final:\n",
    "  print(_node.text)\n",
    "  print(_node.score)\n",
    "  print(\"\")\n",
    "# 31 ,35 , 50 , 40, 42  initial rankings\n",
    "# 35 , 50 , 35 , 40 , 42  final rankings after re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(nodes_final):\n",
    " top_k = 5\n",
    " top_chunks = nodes_final[:top_k]\n",
    " context = \"\\n\\n\".join([node.text for node in top_chunks])\n",
    " prompt = f\"\"\"Answer the following question using the context provided.\n",
    "\n",
    " Question:\n",
    " {query}\n",
    "\n",
    " Context:\n",
    " {context}\n",
    "\n",
    " Answer:\"\"\"\n",
    "\n",
    " answer = llm.invoke(prompt)\n",
    " return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The COCOMO model, which stands for COnstructive COst MOdel, was proposed by Boehm. It is used for software cost estimation and divides software product developments into three categories: Organic, Semidetached, and Embedded. Software cost estimation through COCOMO is done in three stages: Basic COCOMO, Intermediate COCOMO, and Complete COCOMO. The model suggests that effort is somewhat super-linear in problem size, and development time does not increase linearly with product size.\n"
     ]
    }
   ],
   "source": [
    "answer=generate_answer(nodes_final)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Rayleigh curve represents the number of full-time personnel required at any time during a development project. It is specified by two parameters: `td`, the time at which the curve reaches its maximum, and `K`, the total area under the curve. The curve shows that a very small number of engineers are needed at the beginning of a project for planning and specification, and as the project progresses, the number of engineers slowly increases and reaches a peak. After this peak, the number of project staff falls.\n",
      "\n",
      "Its significance is that it models the non-constant staffing levels required for development projects. Norden observed in 1958 that it represents the number of full-time personnel needed. The time `td` when the Rayleigh curve reaches its maximum value corresponds to system testing and product release. It also indicates that approximately 40% of the area under the curve (representing effort/personnel) is to the left of `td` and 60% is to the right.\n"
     ]
    }
   ],
   "source": [
    "query=\"what is a rayleigh curve and what is its significance ?\"\n",
    "nodes=retriever.retrieve(query)\n",
    "embedding_pairs= generate_hypothetical_query_embeddings(nodes,llm,embed_model)\n",
    "query_embedding=embed_model.get_text_embedding(query)\n",
    "nodes_final=compute_final_scores(nodes,embedding_pairs,query_embedding)\n",
    "answer=generate_answer(nodes_final)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putnam's work, beginning in 1976, focused on the problem of staffing software projects. He observed that the level of effort in software development efforts has a similar envelope and found that the Rayleigh-Norden curve relates the number of delivered lines of code to effort and development time.\n",
      "\n",
      "Through analysis of numerous army projects, Putnam derived the expression: L=(Ck K^(1/3) td^(4/3)). In this formula, L represents the size in KLOC, K is the effort expended, and td is the time to develop the software. Ck is a state of technology constant that reflects factors affecting programmer productivity, with values ranging from 2 for a poor development environment (no methodology, poor documentation) to 8 for a good one (software engineering principles used), and 11 for an excellent environment.\n",
      "\n",
      "Putnam also observed that the Rayleigh curve reaches its maximum value at the time of system testing and product release, after which project staff numbers decrease until product installation and delivery.\n",
      "\n",
      "His estimation model indicates an extreme penalty for schedule compression and a significant reward for expanding the schedule. While the Putnam model works reasonably well for very large systems, it seriously overestimates the effort required for medium and small systems.\n"
     ]
    }
   ],
   "source": [
    "query=\"describe Putnam’s Work\"\n",
    "nodes=retriever.retrieve(query)\n",
    "embedding_pairs= generate_hypothetical_query_embeddings(nodes,llm,embed_model)\n",
    "query_embedding=embed_model.get_text_embedding(query)\n",
    "nodes_final=compute_final_scores(nodes,embedding_pairs,query_embedding)\n",
    "answer=generate_answer(nodes_final)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delphi Estimation is a heuristic technique used for cost estimation. It involves a team of experts and a coordinator. Experts carry out estimations independently, mentioning the rationale behind their estimation. The coordinator notes down any extraordinary rationale and circulates it among the experts. Experts then re-estimate. A key characteristic is that experts never meet each other to discuss their viewpoints. This method overcomes some of the problems of individual bias found in simple expert judgment.\n"
     ]
    }
   ],
   "source": [
    "query=\"explain Delphi Estimation\"\n",
    "nodes=retriever.retrieve(query)\n",
    "embedding_pairs= generate_hypothetical_query_embeddings(nodes,llm,embed_model)\n",
    "query_embedding=embed_model.get_text_embedding(query)\n",
    "nodes_final=compute_final_scores(nodes,embedding_pairs,query_embedding)\n",
    "answer=generate_answer(nodes_final)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heuristic techniques are defined as an educated guess based on past experience, or systematic guesses by experts.\n",
      "\n",
      "The specific heuristic techniques discussed are:\n",
      "\n",
      "*   **Expert Judgement:** This is described as an euphemism for a guess made by an expert and suffers from individual bias.\n",
      "*   **Delphi Estimation:** This technique is mentioned as overcoming some of the problems associated with expert judgement.\n"
     ]
    }
   ],
   "source": [
    "query=\"tell about Heuristic techniques discussed in the PDF\"\n",
    "nodes=retriever.retrieve(query)\n",
    "embedding_pairs= generate_hypothetical_query_embeddings(nodes,llm,embed_model)\n",
    "query_embedding=embed_model.get_text_embedding(query)\n",
    "nodes_final=compute_final_scores(nodes,embedding_pairs,query_embedding)\n",
    "answer=generate_answer(nodes_final)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the different Empirical Estimation Techniques are:\n",
      "\n",
      "*   **Single Variable Model**\n",
      "*   **Multivariable Model**\n",
      "*   **COCOMO** (mentioned as an example of an empirical technique)\n"
     ]
    }
   ],
   "source": [
    "query=\"what are different Empirical Estimation Techniques ?\"\n",
    "nodes=retriever.retrieve(query)\n",
    "embedding_pairs= generate_hypothetical_query_embeddings(nodes,llm,embed_model)\n",
    "query_embedding=embed_model.get_text_embedding(query)\n",
    "nodes_final=compute_final_scores(nodes,embedding_pairs,query_embedding)\n",
    "answer=generate_answer(nodes_final)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Point Metric (FP) is a software size estimation metric proposed by Albrecht at IBM in 1979. It was developed to overcome some of the shortcomings of the Lines of Code (LOC) metric.\n",
      "\n",
      "**How it is useful in software engineering:**\n",
      "*   It overcomes some of the shortcomings of the LOC metric.\n",
      "*   Proponents claim it is language independent, meaning it can be applied regardless of the programming language used.\n",
      "*   Proponents also claim that the software size can be easily derived from the problem description, making it useful in early stages of development.\n",
      "\n",
      "The metric is calculated by counting various functional components of a software system, such as inputs, outputs, inquiries, files, and interfaces. These counts can then be multiplied by predefined weights (which vary based on the complexity of each component – Low, Average, High) and summed to arrive at the Function Point value. For example, a simplified formula is FP = 4 #inputs + 5 #Outputs + 4 #inquiries + 10 #files + 7 #interfaces.\n"
     ]
    }
   ],
   "source": [
    "query=\"what is Function Point Metric? and how is it useful in software engineering ?\"\n",
    "nodes=retriever.retrieve(query)\n",
    "embedding_pairs= generate_hypothetical_query_embeddings(nodes,llm,embed_model)\n",
    "query_embedding=embed_model.get_text_embedding(query)\n",
    "nodes_final=compute_final_scores(nodes,embedding_pairs,query_embedding)\n",
    "answer=generate_answer(nodes_final)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided:\n",
      "\n",
      "**Jensen Model:**\n",
      "The Jensen model is an estimation technique that is very similar to the Putnam model. It aims to soften the effect of schedule compression on effort, making it applicable to smaller and medium-sized projects. Jensen proposed the equation:\n",
      "`L = Cte * td * K^(1/2)`\n",
      "Where:\n",
      "*   `L` is the lines of code (implicitly, as it's the output of the equation).\n",
      "*   `Cte` is the effective technology constant.\n",
      "*   `td` is the time to develop the software.\n",
      "*   `K` is the effort needed to develop the software.\n",
      "\n",
      "**Comparison to other models in the PDF:**\n",
      "\n",
      "*   **Compared to Putnam Model:** The Jensen model is explicitly stated to be \"very similar to Putnam model.\"\n",
      "*   **Compared to Empirical Estimation Techniques (Single Variable and Multivariable Models):**\n",
      "    *   The Jensen model is an empirical estimation technique that uses an equation with multiple variables (`Cte`, `td`, `K`) to estimate `L`.\n",
      "    *   **Single Variable Model:** Estimates a parameter based on one characteristic (`Parameter = C1(Characteristic)d1`). Jensen's model uses multiple input variables, so it's not a single variable model.\n",
      "    *   **Multivariable Model:** Assumes the parameter to be estimated depends on more than one characteristic (`Parameter = C1(Char)d1 + C2(Char)d2 + ...`). While Jensen's model uses multiple variables, its specific form (`L=CtetdK1/2`) is a multiplicative relationship rather than an additive one as shown in the general multivariable model example. However, it falls under the broader category of empirical models that use multiple factors. Multivariable models are generally considered more accurate than single variable models.\n",
      "*   **Compared to Halstead's Software Science:**\n",
      "    *   Both Jensen's model and Halstead's Software Science are analytical techniques used for estimation.\n",
      "    *   Jensen's model provides an equation relating lines of code, technology constant, development time, and effort.\n",
      "    *   Halstead's Software Science is an analytical technique specifically designed to estimate size, development effort, and development time.\n",
      "*   **Compared to Staffing Level Estimation (Norden/Rayleigh Curve):**\n",
      "    *   Jensen's model focuses on estimating software development parameters like lines of code, time, and effort.\n",
      "    *   Staffing Level Estimation, as observed by Norden, focuses specifically on the number of personnel required during a development project over time, often represented by a Rayleigh curve. These are different types of estimations, with Jensen focusing on project characteristics and Staffing Level focusing on resource allocation over time.\n"
     ]
    }
   ],
   "source": [
    "query=\"tell about Jensen Model? compare it to other models in the PDF\"\n",
    "nodes=retriever.retrieve(query)\n",
    "embedding_pairs= generate_hypothetical_query_embeddings(nodes,llm,embed_model)\n",
    "query_embedding=embed_model.get_text_embedding(query)\n",
    "nodes_final=compute_final_scores(nodes,embedding_pairs,query_embedding)\n",
    "answer=generate_answer(nodes_final)\n",
    "print(answer.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
