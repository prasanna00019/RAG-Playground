{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Self-RAG \n",
    "\n",
    "Welcome to the **Self-Retrieval-Augmented Generation (Self-RAG)** project!  \n",
    "This notebook demonstrates a workflow inspired by the [Self-RAG paper (arXiv:2310.11511)](https://arxiv.org/pdf/2310.11511), where a language model decides **when and how** to retrieve external knowledge to answer queries more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Overview\n",
    "\n",
    "- **Self-RAG** enables an LLM to:\n",
    "    - Decide if external retrieval is needed for a given query.\n",
    "    - Retrieve relevant documents using a vector store.\n",
    "    - Filter and retain only the most relevant context.\n",
    "    - Generate answers using either retrieved context or internal knowledge.\n",
    "    - Evaluate the factual support and usefulness of generated responses.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Workflow\n",
    "\n",
    "1. **API Key Setup**  \n",
    "     Securely input your API keys for Groq and Google.\n",
    "\n",
    "2. **Model Initialization**  \n",
    "     - Uses `langchain-groq` and `langchain-google-genai` for LLMs.\n",
    "     - Uses Google GenAI embeddings for vector search.\n",
    "\n",
    "3. **Document Loading & Indexing**  \n",
    "     - Loads documents from the `data` directory.\n",
    "     - Splits them into chunks and indexes them in a vector store.\n",
    "\n",
    "4. **Self-RAG Pipeline**  \n",
    "     - `retrieval_required`: Determines if retrieval is needed.\n",
    "     - `is_context_relevant`: Filters retrieved chunks based on relevance.\n",
    "     - `response_generation`: Generates answers using either context or model knowledge.\n",
    "     - `support_response` & `utility_response`: \n",
    "       - Evaluate how well the generated answer is supported by context (if retrieved).\n",
    "       - Always prints the **utility score** (even if no retrieval is used), as described in the Self-RAG paper.\n",
    "     - `self_RAG`: Coordinates all the steps for each query.\n",
    "\n",
    "5. **Example Queries**  \n",
    "     - The notebook runs queries related to robotics and shows how Self-RAG handles them.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Key Features\n",
    "\n",
    "- **Dynamic Retrieval**: Avoids unnecessary lookups.\n",
    "- **Context Filtering**: Keeps only what's truly relevant.\n",
    "- **Answer Scoring**: Scores responses based on factual support and usefulness.\n",
    "- **Modular Design**: Clean, reusable components.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References\n",
    "\n",
    "- [Self-RAG: Learning to Retrieve, Generate, and Critique in a Self-Refining Loop (arXiv:2310.11511)](https://arxiv.org/pdf/2310.11511)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° How to Use\n",
    "\n",
    "1. Place your documents in the `data` folder.\n",
    "2. Run the notebook cells sequentially.\n",
    "3. Enter your API keys when prompted.\n",
    "4. Run your own queries using the `self_RAG` function.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "- ‚ö†Ô∏è **This is not an exact reproduction of the Self-RAG paper**, but a simplified and practical implementation inspired by its core ideas.\n",
    "- The current version avoids agentic loop structures and focuses on step-wise reasoning with modular functions.\n",
    "- The **utility score is printed for every response**, even when no retrieval is performed, in alignment with the critique step in the paper.\n",
    "- Feel free to adapt or extend it for your own use cases!\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Happy experimenting! ü§ó\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image reference : [Langchain blog](https://blog.langchain.com/agentic-rag-with-langgraph/)\n",
    "Paper Reference : [Self RAG paper](https://arxiv.org/pdf/2310.11511)\n",
    "![Model](https://blog.langchain.com/content/images/2024/02/data-src-image-ebf55e8c-de51-49b8-9f32-94ccbf24741f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Groq API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GOOGLE_API_KEY = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm_2 = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "splitter = SentenceSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "embed_model = GoogleGenAIEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    embed_batch_size=100,\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_required(query):\n",
    "    \"\"\"\n",
    "    For a given query, determine if retrieval is necessary using the LLM.\n",
    "    Returns True if 'Yes', False otherwise.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the query: \"{query}\", determine if retrieval of external knowledge is required\n",
    "    to accurately answer it. Reply with only one word: Yes or No.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    answer = response.content.strip().lower()\n",
    "    print(answer)\n",
    "    if \"yes\" in answer:\n",
    "        return True\n",
    "    elif \"no\" in answer:\n",
    "        return False\n",
    "    else:\n",
    "        # fallback case\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_context_relevant(query, context):\n",
    "    \"\"\"\n",
    "    For a given query and context chunk, determine if the context is relevant.\n",
    "    Returns True if relevant, False otherwise.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Query: \"{query}\"\n",
    "    Context: \"{context}\"\n",
    "\n",
    "    Is the context relevant to the query? Reply with only one word: Relevant or Irrelevant.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_2.invoke(prompt)\n",
    "    answer = response.content.strip().lower()\n",
    "    print(f\"LLM response: {answer}\")\n",
    "\n",
    "    if \"relevant\" in answer and \"irrelevant\" not in answer:\n",
    "        return True\n",
    "    elif \"irrelevant\" in answer:\n",
    "        return False\n",
    "    else:\n",
    "        # fallback for unclear answers\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_generation(query, context):\n",
    "    \"\"\"\n",
    "    Generate a response for a given query using the context.\n",
    "    \"\"\"\n",
    "    if(context==\"No retrieval needed\"):\n",
    "      prompt= f\"\"\"\n",
    "      Answer the following {query} based on your knowledge\n",
    "      \"\"\"\n",
    "      response = llm_2.invoke(prompt)\n",
    "      return response.content.strip()\n",
    "    else:\n",
    "      prompt = f\"\"\"\n",
    "      You are given the following context: \"{context}\"\n",
    "\n",
    "      Now answer the following query using only the information from the context:\n",
    "      \"{query}\"\n",
    "\n",
    "      If the answer is not found in the context, say 'Insufficient information.'\n",
    "      \"\"\"\n",
    "      response = llm.invoke(prompt)\n",
    "      return response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_response(response, context):\n",
    "    \"\"\"\n",
    "    Determines if response is supported by the given context.\n",
    "    Returns: 'Fully supported', 'Partially supported', or 'No support'\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Context: \"{context}\"\n",
    "    Response: \"{response}\"\n",
    "\n",
    "    Is the response supported by the context?\n",
    "    Reply with only one of the following: Fully supported, Partially supported, or No support.\n",
    "    \"\"\"\n",
    "    result = llm_2.invoke(prompt).content.strip().lower()\n",
    "\n",
    "    if \"fully\" in result:\n",
    "        return \"Fully supported\"\n",
    "    elif \"partially\" in result:\n",
    "        return \"Partially supported\"\n",
    "    elif \"no support\" in result or \"not supported\" in result:\n",
    "        return \"No support\"\n",
    "    else:\n",
    "        return \"Unclear\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_response(query, response):\n",
    "    \"\"\"\n",
    "    Rates the utility of the response from 1 (poor) to 5 (excellent).\n",
    "    Returns an integer score.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Query: \"{query}\"\n",
    "    Response: \"{response}\"\n",
    "\n",
    "    On a scale of 1 to 5, how useful is this response in answering the query?\n",
    "    Reply with only the number.\n",
    "    \"\"\"\n",
    "    result = llm_2.invoke(prompt).content.strip()\n",
    "\n",
    "    try:\n",
    "        score = int(result)\n",
    "        return min(max(score, 1), 5)\n",
    "    except ValueError:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_RAG(query, retriever):\n",
    "    #  Check if retrieval is necessary\n",
    "    if retrieval_required(query):\n",
    "        print(\"üîç Retrieval required. Fetching documents...\")\n",
    "        docs = retriever.retrieve(query)\n",
    "        contexts = [d.text for d in docs]\n",
    "\n",
    "        #  Filter relevant contexts\n",
    "        relevant_contexts = []\n",
    "        for i, c in enumerate(contexts):\n",
    "            if is_context_relevant(query, c):\n",
    "                print(f\"‚úÖ Document {i+1} is relevant.\")\n",
    "                relevant_contexts.append(c)\n",
    "            else:\n",
    "                print(f\"‚ùå Document {i+1} is not relevant.\")\n",
    "\n",
    "        #  Handle no relevant context\n",
    "        if not relevant_contexts:\n",
    "            print(\"‚ö†Ô∏è No relevant context found. Generating without context...\")\n",
    "            return response_generation(query, \"No relevant context found.\")\n",
    "\n",
    "        #  Generate responses and evaluate\n",
    "        responses = []\n",
    "        for i, context in enumerate(relevant_contexts):\n",
    "            _response = response_generation(query, context)\n",
    "            _support = support_response(_response, context).strip().lower()\n",
    "            try:\n",
    "                _utility = utility_response(query, _response)\n",
    "            except:\n",
    "                _utility = 0\n",
    "\n",
    "            responses.append((_response, _support, _utility))\n",
    "\n",
    "        # Select best response (fully supported & highest utility)\n",
    "        final_best_answer = max(\n",
    "            responses, key=lambda x: (x[1] == 'fully supported', x[2])\n",
    "        )\n",
    "        print(f\"\\nüèÅ Selected Best Response: [Support: {final_best_answer[1]}, Utility: {final_best_answer[2]}]\")\n",
    "        return final_best_answer[0]\n",
    "\n",
    "    else:\n",
    "        print(\"üß† Retrieval not required. Generating without documents...\")\n",
    "        response= response_generation(query, \"No retrieval needed\")\n",
    "        utility_raw = utility_response(query, response)\n",
    "        print(f\"‚úÖ Direct Response Utility Score: {utility_raw}\")\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "üß† Retrieval not required. Generating without documents...\n",
      "‚úÖ Direct Response Utility Score: 5\n",
      "Degrees of freedom (DOF) in robotics refer to the number of independent movements a robot can perform. Each DOF corresponds to a specific direction or type of movement, such as translation along an axis or rotation around it. In three-dimensional space, there are six possible DOF: three translational (movement along x, y, and z axes) and three rotational (roll, pitch, and yaw).\n",
      "\n",
      "Robots can have varying numbers of DOF depending on their design and purpose. For example, a simple robotic arm might have 3 DOF, allowing it to move in three planes, while a more complex robot might have 6 DOF, enabling it to move freely in all directions. Each joint in a robot contributes to its total DOF, with different types of joints offering different numbers of DOF. For instance, a revolute joint provides 1 DOF, while a spherical joint offers 3 DOF.\n",
      "\n",
      "The total DOF of a robot is typically the sum of the DOF from each of its joints. However, the robot's base and environment can also affect its DOF. A fixed base limits translational movement, whereas a mobile platform adds translational DOF.\n",
      "\n",
      "The number of DOF significantly impacts a robot's capabilities. Higher DOF robots can perform complex tasks requiring precise and varied movements, such as surgery or assembly. In contrast, robots with fewer DOF are often used for repetitive tasks where movement is more constrained, like welding or picking objects.\n",
      "\n",
      "Control complexity increases with more DOF, affecting the robot's cost, size, and energy use. Additionally, some DOF may be active (controlled by actuators) or passive (allowing movement without active control), which is important for tasks requiring environmental interaction, such as grasping objects with a robotic hand.\n",
      "\n",
      "In essence, DOF determine a robot's mobility and versatility, balancing between task flexibility and control complexity.\n"
     ]
    }
   ],
   "source": [
    "final_SELF_RAG_response=self_RAG(\"what are degrees of freedom in a robot ?\",retriever)\n",
    "print(final_SELF_RAG_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "üîç Retrieval required. Fetching documents...\n",
      "LLM response: relevant\n",
      "‚úÖ Document 1 is relevant.\n",
      "LLM response: irrelevant\n",
      "‚ùå Document 2 is not relevant.\n",
      "LLM response: irrelevant\n",
      "‚ùå Document 3 is not relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 4 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 5 is relevant.\n",
      "\n",
      "üèÅ Selected Best Response: [Support: fully supported, Utility: 4]\n",
      "Several advanced actuator technologies are being developed and applied in robotics to address limitations of conventional systems or to enable new capabilities:\n",
      "\n",
      "*   **Shape memory alloy actuators** use materials that change shape when heated, providing compact actuators with high force output. They are valuable for applications requiring small size, silent operation, or biomimetic behavior, but typically have slow response times and limited stroke length.\n",
      "*   **Electroactive polymers** change shape when subjected to electric fields, potentially providing artificial muscles with characteristics similar to biological systems. These materials offer the possibility of very high power-to-weight ratios and biomimetic behavior but are still largely in the research phase.\n",
      "*   **Piezoelectric actuators** use materials that deform when subjected to electric fields, providing very precise positioning with fast response times. They are commonly used in precision positioning systems and active vibration control but typically provide only small displacements.\n",
      "*   **Magnetostrictive actuators** use materials that change shape in magnetic fields, providing precise positioning with good force output. They offer excellent resolution and response time but require sophisticated control systems and are typically limited to small displacements.\n"
     ]
    }
   ],
   "source": [
    "query_2=\"tell about Advanced and Emerging Actuator Technologies\"\n",
    "response_2=self_RAG(query_2,retriever)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "üîç Retrieval required. Fetching documents...\n",
      "LLM response: relevant\n",
      "‚úÖ Document 1 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 2 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 3 is relevant.\n",
      "LLM response: irrelevant\n",
      "‚ùå Document 4 is not relevant.\n",
      "LLM response: irrelevant\n",
      "‚ùå Document 5 is not relevant.\n",
      "\n",
      "üèÅ Selected Best Response: [Support: fully supported, Utility: 3]\n",
      "Cloud and Edge Computing is integrating into robots in the following ways:\n",
      "*   **Cloud robotics** allows robots to access powerful computational resources and shared knowledge bases through network connections.\n",
      "*   **Edge computing** provides local processing capabilities.\n"
     ]
    }
   ],
   "source": [
    "query_3=\"how is Cloud and Edge Computing integrating into robots\"\n",
    "response_3=self_RAG(query_3,retriever)\n",
    "print(response_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "üîç Retrieval required. Fetching documents...\n",
      "LLM response: relevant\n",
      "‚úÖ Document 1 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 2 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 3 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 4 is relevant.\n",
      "LLM response: irrelevant\n",
      "‚ùå Document 5 is not relevant.\n",
      "\n",
      "üèÅ Selected Best Response: [Support: fully supported, Utility: 5]\n",
      "The Technological Convergence and Breakthrough Potential discussed include:\n",
      "\n",
      "*   **Artificial intelligence**: advancements in machine learning, neural networks, cognitive architectures, and the integration of large language models for sophisticated reasoning, decision-making, and natural human-robot communication.\n",
      "*   **Quantum computing**: potential to revolutionize robot control and planning algorithms by solving complex optimization problems and providing unprecedented sensitivity and precision for robot perception systems through quantum sensors.\n",
      "*   **Biotechnology and robotics convergence**: creation of new possibilities for bio-hybrid systems combining biological and artificial components.\n",
      "*   **Advanced materials science**: production of new materials like self-healing materials, programmable matter, and materials with extreme strength-to-weight ratios to transform robot design.\n"
     ]
    }
   ],
   "source": [
    "query_4=\"what are the Technological Convergence and Breakthrough Potential discussed ?\"\n",
    "response_4=self_RAG(query_4,retriever)\n",
    "print(response_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "üß† Retrieval not required. Generating without documents...\n",
      "‚úÖ Direct Response Utility Score: 5\n",
      "Actuators are devices that convert energy into motion, essential in various systems like robotics, machines, and automation. Here's an organized overview of different types of actuators, their mechanisms, applications, and characteristics:\n",
      "\n",
      "1. **Hydraulic Actuators**\n",
      "   - **Mechanism:** Use fluid pressure to create movement via a piston in a cylinder.\n",
      "   - **Applications:** Heavy machinery, construction equipment, automotive brakes.\n",
      "   - **Pros/Cons:** High strength, but prone to fluid leaks.\n",
      "\n",
      "2. **Pneumatic Actuators**\n",
      "   - **Mechanism:** Utilize compressed air or gas to move a piston.\n",
      "   - **Applications:** Factories, conveyor belts, automation.\n",
      "   - **Pros/Cons:** Clean and lightweight, but less precise.\n",
      "\n",
      "3. **Electric Actuators**\n",
      "   - **Mechanism:** Convert electrical energy into motion using motors.\n",
      "   - **Applications:** Fans, robotics, automation.\n",
      "   - **Pros/Cons:** Versatile and precise, but can be heavy.\n",
      "\n",
      "4. **Piezoelectric Actuators**\n",
      "   - **Mechanism:** Change shape with electric charge using piezoelectric materials.\n",
      "   - **Applications:** Microscopes, nanotechnology.\n",
      "   - **Pros/Cons:** High precision, limited movement range.\n",
      "\n",
      "5. **Shape Memory Alloy (SMA) Actuators**\n",
      "   - **Mechanism:** Return to original shape when heated.\n",
      "   - **Applications:** Medical devices, smart textiles.\n",
      "   - **Pros/Cons:** Simple and lightweight, slower response.\n",
      "\n",
      "6. **Hybrid Actuators**\n",
      "   - **Mechanism:** Combine different actuator types for enhanced performance.\n",
      "   - **Applications:** High-performance systems needing balance.\n",
      "   - **Pros/Cons:** Balanced strength and precision, complex design.\n",
      "\n",
      "7. **Electroactive Polymer (EAP) Actuators**\n",
      "   - **Mechanism:** Change shape with electric fields.\n",
      "   - **Applications:** Soft robotics, wearable devices.\n",
      "   - **Pros/Cons:** Flexible, but may lack strength.\n",
      "\n",
      "8. **Thermal Actuators**\n",
      "   - **Mechanism:** Expand/contract with temperature changes.\n",
      "   - **Applications:** Valves, simple on/off mechanisms.\n",
      "   - **Pros/Cons:** Simple, but slow response.\n",
      "\n",
      "9. **Magnetic Actuators**\n",
      "   - **Mechanism:** Use magnetic fields for movement.\n",
      "   - **Applications:** Contactless applications, sensors.\n",
      "   - **Pros/Cons:** Fast, but limited force.\n",
      "\n",
      "Each actuator type offers unique advantages, making them suitable for specific applications. Understanding their mechanisms and characteristics helps in selecting the right actuator for a given task.\n"
     ]
    }
   ],
   "source": [
    "query_5=\"what are different types of actuators ? explain them\"\n",
    "response_5=self_RAG(query_5,retriever)\n",
    "print(response_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "üîç Retrieval required. Fetching documents...\n",
      "LLM response: relevant\n",
      "‚úÖ Document 1 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 2 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 3 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 4 is relevant.\n",
      "LLM response: irrelevant\n",
      "‚ùå Document 5 is not relevant.\n",
      "\n",
      "üèÅ Selected Best Response: [Support: fully supported, Utility: 5]\n",
      "The context provides two classifications of robots:\n",
      "\n",
      "1.  **By Configuration:**\n",
      "    *   Cylindrical robots\n",
      "    *   SCARA robots\n",
      "    *   Parallel robots (including Delta robots as a specific type)\n",
      "\n",
      "2.  **By Locomotion Method:**\n",
      "    *   Wheeled robots\n",
      "    *   Tracked robots\n",
      "    *   Legged robots\n",
      "    *   Flying robots\n",
      "    *   Swimming robots\n"
     ]
    }
   ],
   "source": [
    "query_6=\"what are different classifications of robots as said in PDF  ?\"\n",
    "response_6=self_RAG(query_6,retriever)\n",
    "print(response_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "üß† Retrieval not required. Generating without documents...\n",
      "‚úÖ Direct Response Utility Score: 5\n",
      "**Visual Perception Systems in Robots: An Overview**\n",
      "\n",
      "**Introduction:**\n",
      "Visual perception in robots refers to the ability of robots to interpret and understand visual data from their environment, enabling them to perform tasks that require awareness and interaction with their surroundings.\n",
      "\n",
      "**Key Components:**\n",
      "1. **Sensors:** Robots use cameras or other visual sensors to capture images or video streams.\n",
      "2. **Processing:** Software and algorithms, such as convolutional neural networks (CNNs), analyze the captured data.\n",
      "\n",
      "**Functions:**\n",
      "1. **Object Recognition:** Identifying objects like cats or chairs using CNNs.\n",
      "2. **Object Detection:** Locating objects within images using tools like YOLO or SSD.\n",
      "3. **Scene Understanding:** Interpreting entire environments, such as kitchens or streets.\n",
      "4. **Object Tracking:** Following moving objects using methods like optical flow.\n",
      "5. **Depth Perception:** Using stereo vision or LiDAR for navigation.\n",
      "6. **Segmentation:** Dividing images into parts, with instance segmentation for individual objects.\n",
      "\n",
      "**Techniques:**\n",
      "- **Visual Odometry:** Tracking movement for localization.\n",
      "- **SLAM (Simultaneous Localization and Mapping):** Combining localization with environment mapping.\n",
      "\n",
      "**Applications:**\n",
      "1. **Pick and Place:** Identifying and grasping objects.\n",
      "2. **Autonomous Vehicles:** Detecting pedestrians and traffic elements.\n",
      "3. **Healthcare:** Assisting in surgeries or navigation.\n",
      "4. **Industrial Robots:** Inspecting products for quality control.\n",
      "\n",
      "**Challenges:**\n",
      "1. **Real-Time Processing:** Need for quick reactions.\n",
      "2. **Lighting Variations:** Adapting to different conditions.\n",
      "3. **Occlusion:** Handling blocked views.\n",
      "4. **Generalization:** Adapting to new environments.\n",
      "\n",
      "**Future Trends:**\n",
      "1. **Efficient Neural Networks:** Models like MobileNet or EfficientNet.\n",
      "2. **Multi-Modal Systems:** Integrating visual data with other senses.\n",
      "3. **Edge Computing:** Local processing for speed and security.\n",
      "\n",
      "This structured approach provides a comprehensive understanding of visual perception systems in robots, highlighting their components, functions, techniques, applications, challenges, and future directions.\n"
     ]
    }
   ],
   "source": [
    "query_7=\"Visual Perception Systems in robots\"\n",
    "response_7=self_RAG(query_7,retriever)\n",
    "print(response_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "üîç Retrieval required. Fetching documents...\n",
      "LLM response: relevant\n",
      "‚úÖ Document 1 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 2 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 3 is relevant.\n",
      "LLM response: irrelevant\n",
      "‚ùå Document 4 is not relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 5 is relevant.\n",
      "\n",
      "üèÅ Selected Best Response: [Support: fully supported, Utility: 3]\n",
      "Design Principles for Human-Robot Interaction, as described in the context, include:\n",
      "\n",
      "*   **User-centered design:** This approach places human users at the center of the design process, focusing on meeting human needs, understanding user requirements, preferences, and limitations.\n",
      "*   **Transparency and explainability:** This ensures that humans can understand robot behavior and decision-making processes, which is vital for building trust and enabling appropriate human oversight.\n",
      "*   **Adaptability and personalization:** This allows robots to adjust their behavior to individual users and changing contexts, crucial for maintaining effective interactions over time.\n"
     ]
    }
   ],
   "source": [
    "query_8=\"Design Principles for Human-Robot Interaction as described in PDF\"\n",
    "response_8=self_RAG(query_8,retriever)\n",
    "print(response_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "üîç Retrieval required. Fetching documents...\n",
      "LLM response: relevant\n",
      "‚úÖ Document 1 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 2 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 3 is relevant.\n",
      "LLM response: irrelevant\n",
      "‚ùå Document 4 is not relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 5 is relevant.\n",
      "\n",
      "üèÅ Selected Best Response: [Support: fully supported, Utility: 5]\n",
      "Bio-inspired materials derived from natural systems are providing new capabilities for robotics. Examples include gecko-inspired adhesives for climbing robots, shark-skin-inspired surfaces to reduce drag for underwater robots, and self-healing materials that could enable robots to repair themselves after damage.\n",
      "\n",
      "Advanced manufacturing techniques, including 3D printing, additive manufacturing, and automated assembly, are reducing the cost and complexity of robot production. These techniques also enable mass customization, rapid prototyping, and the creation of complex structures that would be impossible with traditional manufacturing methods.\n"
     ]
    }
   ],
   "source": [
    "query_9=\"Advanced Materials and Manufacturing for robots\"\n",
    "response_9=self_RAG(query_9,retriever)\n",
    "print(response_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "üîç Retrieval required. Fetching documents...\n",
      "LLM response: relevant\n",
      "‚úÖ Document 1 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 2 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 3 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 4 is relevant.\n",
      "LLM response: relevant\n",
      "‚úÖ Document 5 is relevant.\n",
      "\n",
      "üèÅ Selected Best Response: [Support: fully supported, Utility: 5]\n",
      "Sensor Technology Advances include:\n",
      "*   **Advanced vision systems** incorporating multispectral imaging, hyperspectral analysis, and computational photography, providing superhuman visual capabilities. Event-based cameras offer fast response times and low power consumption.\n",
      "*   **Tactile sensing technologies** are approaching the sensitivity and resolution of human touch, with distributed tactile sensors providing detailed information about contact forces, surface textures, and object properties. Some systems can even detect chemical properties through artificial smell and taste capabilities.\n",
      "*   **Proprioceptive sensing** is becoming more sophisticated, with high-resolution joint encoders, inertial measurement units, and distributed strain sensors providing detailed information about robot posture and motion.\n",
      "*   **Sensor fusion approaches** combine information from multiple sensor modalities using advanced algorithms to provide more accurate and reliable perception.\n"
     ]
    }
   ],
   "source": [
    "query_10=\"what are the Sensor Technology Advances ?\"\n",
    "response_10=self_RAG(query_10,retriever)\n",
    "print(response_10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
